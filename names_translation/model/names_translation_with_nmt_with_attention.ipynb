{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "names_translation_with_nmt_with_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomas-chauvet/names_transliteration/blob/master/names_translation/model/names_translation_with_nmt_with_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Names translation - Neural machine translation with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CiwtNgENbx2g"
      },
      "source": [
        "This notebook trains a sequence to sequence (seq2seq) model to translitate names with arabic characters to names in latin character. Usually we call this task \"romanization\". It is the task to transform string from one alphabet to latin alphanet.\n",
        "\n",
        "After training the model in this notebook, you will be able to input an arabic name, such as *محمد‎*, and return the transliteration/translation of this name: *mohammad*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyy8k2VCkTTs",
        "colab_type": "text"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import json\n",
        "import time\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "import logging\n",
        "import string\n",
        "from typing import List, Union"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## Download and prepare the dataset\n",
        "\n",
        "We'll use three datasets:\n",
        "*   Google transliteration dataset from repository on [github](https://github.com/google/transliteration). Example: *عادل\tadel*\n",
        "*   ANETAC dataset on [github](https://github.com/MohamedHadjAmeur/ANETAC). Example: *PERSON Adel اديل*. For this dataset we'll filter on *PERSON* only,\n",
        "*   NETranliteration COLING 2018 dataset on [github](https://github.com/steveash/NETransliteration-COLING2018/blob/master/data/wd_arabic.normalized.aligned.tokens).\n",
        "\n",
        "We already cleaned and prepared a concatenated dataset in a dedicated [repository](https://github.com/thomas-chauvet/names_transliteration).\n",
        "\n",
        "After downloading the dataset, we will clean them and concatenate the two in one.\n",
        "\n",
        "This notebook is based on Tensorflow tutorial [Neural machine translation with attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPKlrFuX9Ygh",
        "colab_type": "text"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRmz8Ewr66Sb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "96b1ad26-8caa-40a2-f802-7fb7b4e63af8"
      },
      "source": [
        "data_path = Path.cwd().parent / \"data\"\n",
        "data_path.mkdir(parents=True, exist_ok=True)\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/thomas-chauvet/names_transliteration/master/data/clean/arabic_english.csv\")\n",
        "df.to_csv(data_path / \"ar2en.csv\", index=False)\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>arabic</th>\n",
              "      <th>english</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>العالي</td>\n",
              "      <td>aal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>اعشي</td>\n",
              "      <td>asha</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>اعثم</td>\n",
              "      <td>atham</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>اا</td>\n",
              "      <td>aa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ادلاند</td>\n",
              "      <td>aadland</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   arabic  english\n",
              "0  العالي      aal\n",
              "1    اعشي     asha\n",
              "2    اعثم    atham\n",
              "3      اا       aa\n",
              "4  ادلاند  aadland"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBAJ4OmR9mtG",
        "colab_type": "text"
      },
      "source": [
        "### Prepare data for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eAY9k49G3jE_",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[\"!\" + w + \"?\" for w in l.split(',')]  for l in lines[:num_examples]]\n",
        "\n",
        "  return zip(*word_pairs)\n",
        "\n",
        "\n",
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  inp_lang, targ_lang = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZCp0v3J-vRv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2d07f701-3be5-4e55-beb5-57d25b8fc489"
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = None\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(data_path / \"ar2en.csv\", num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
        "\n",
        "print(f\"Size of the longest string for arabic is {max_length_inp}\")\n",
        "print(f\"Size of the longest string for english is {max_length_targ}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the longest string for arabic is 26\n",
            "Size of the longest string for english is 27\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4QILQkOs3jFG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3aea8fdd-d2a0-4494-d137-5238e122c4a5"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94341 94341 23586 23586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lJPmLZGMeD5q",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VXukARTDd7MT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "7ea1f61f-f766-4bb7-ab06-a98331d3b1b7"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> !\n",
            "13 ----> م\n",
            "4 ----> ا\n",
            "6 ----> ن\n",
            "19 ----> ج\n",
            "3 ----> ي\n",
            "11 ----> ت\n",
            "2 ----> ?\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> !\n",
            "13 ----> m\n",
            "3 ----> a\n",
            "6 ----> n\n",
            "26 ----> j\n",
            "4 ----> e\n",
            "4 ----> e\n",
            "11 ----> t\n",
            "2 ----> ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TqHsArVZ3jFS",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qc6-NK1GtWQt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f384e34b-e1b6-4de2-ef95-2934bca3874b"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 26]), TensorShape([64, 27]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TNfHIF71ulLu"
      },
      "source": [
        "## Write the encoder and decoder model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvpopK4Ix4bx",
        "colab_type": "text"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZ2rI24i3jFg",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  # @tf.function(input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPuVxvqxzunH",
        "colab_type": "text"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umohpBN2OM94",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B-bdSxfzz3C",
        "colab_type": "text"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ_B3mhW3jFk",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, inputs, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(inputs)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drNJCr5f9TO5",
        "colab_type": "text"
      },
      "source": [
        "### Instanciate encoder, attention, decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J0OFNc089Vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(vocab_inp_size: int, vocab_tar_size: int, embedding_dim: int, units: int, batch_sz: int):\n",
        "  encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_sz)\n",
        "  attention = BahdanauAttention(10)\n",
        "  decoder = Decoder(vocab_tar_size, embedding_dim, units, batch_sz)\n",
        "  return encoder, attention, decoder"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMLu8z1_-u6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder, attention_layer, decoder = get_model(vocab_inp_size, vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56dKOgYn9Wnw",
        "colab_type": "text"
      },
      "source": [
        "### Check shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28yIoaZa89xQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "8ff257da-edd7-42dc-824a-a63c7c0f9ae0"
      },
      "source": [
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print(\n",
        "    \"Encoder output shape: (batch size, sequence length, units) {}\".format(\n",
        "        sample_output.shape\n",
        "    )\n",
        ")\n",
        "print(\"Encoder Hidden state shape: (batch size, units) {}\".format(sample_hidden.shape))\n",
        "\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\n",
        "    \"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(\n",
        "        attention_weights.shape\n",
        "    )\n",
        ")\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(\n",
        "    tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output\n",
        ")\n",
        "\n",
        "print(\n",
        "    \"Decoder output shape: (batch_size, vocab size) {}\".format(\n",
        "        sample_decoder_output.shape\n",
        "    )\n",
        ")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 26, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 26, 1)\n",
            "Decoder output shape: (batch_size, vocab size) (64, 55)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtuZtvIID422",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "04f6e8f9-4fd9-40f9-b8d2-f2d8dfd03734"
      },
      "source": [
        "encoder.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  14336     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  3938304   \n",
            "=================================================================\n",
            "Total params: 3,952,640\n",
            "Trainable params: 3,952,640\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooSQvPOTD4yP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "5a94599d-7360-455d-dca9-b78ca36b0fa3"
      },
      "source": [
        "decoder.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      multiple                  14080     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  multiple                  7084032   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              multiple                  56375     \n",
            "_________________________________________________________________\n",
            "bahdanau_attention_1 (Bahdan multiple                  2100225   \n",
            "=================================================================\n",
            "Total params: 9,254,712\n",
            "Trainable params: 9,254,712\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmTHr5iV3jFr",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zj8bXQTgNwrF",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hpObfY22IddU"
      },
      "source": [
        "## Training\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sC9ArXSsVfqn",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['!']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ddefjBMa3jF0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc0b5d3b-e29e-40fa-8e46-029e43963a42"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.1597\n",
            "Epoch 1 Batch 100 Loss 0.7472\n",
            "Epoch 1 Batch 200 Loss 0.6967\n",
            "Epoch 1 Batch 300 Loss 0.3040\n",
            "Epoch 1 Batch 400 Loss 0.3284\n",
            "Epoch 1 Batch 500 Loss 0.2817\n",
            "Epoch 1 Batch 600 Loss 0.2748\n",
            "Epoch 1 Batch 700 Loss 0.2781\n",
            "Epoch 1 Batch 800 Loss 0.2133\n",
            "Epoch 1 Batch 900 Loss 0.2316\n",
            "Epoch 1 Batch 1000 Loss 0.2334\n",
            "Epoch 1 Batch 1100 Loss 0.2514\n",
            "Epoch 1 Batch 1200 Loss 0.2082\n",
            "Epoch 1 Batch 1300 Loss 0.2301\n",
            "Epoch 1 Batch 1400 Loss 0.1857\n",
            "Epoch 1 Loss 0.3316\n",
            "Time taken for 1 epoch 295.84784150123596 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.2388\n",
            "Epoch 2 Batch 100 Loss 0.2027\n",
            "Epoch 2 Batch 200 Loss 0.1792\n",
            "Epoch 2 Batch 300 Loss 0.1936\n",
            "Epoch 2 Batch 400 Loss 0.2057\n",
            "Epoch 2 Batch 500 Loss 0.2095\n",
            "Epoch 2 Batch 600 Loss 0.2284\n",
            "Epoch 2 Batch 700 Loss 0.1959\n",
            "Epoch 2 Batch 800 Loss 0.2328\n",
            "Epoch 2 Batch 900 Loss 0.2332\n",
            "Epoch 2 Batch 1000 Loss 0.2263\n",
            "Epoch 2 Batch 1100 Loss 0.2026\n",
            "Epoch 2 Batch 1200 Loss 0.2027\n",
            "Epoch 2 Batch 1300 Loss 0.1991\n",
            "Epoch 2 Batch 1400 Loss 0.1756\n",
            "Epoch 2 Loss 0.2093\n",
            "Time taken for 1 epoch 269.33190083503723 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.1810\n",
            "Epoch 3 Batch 100 Loss 0.1754\n",
            "Epoch 3 Batch 200 Loss 0.1963\n",
            "Epoch 3 Batch 300 Loss 0.1963\n",
            "Epoch 3 Batch 400 Loss 0.1920\n",
            "Epoch 3 Batch 500 Loss 0.1749\n",
            "Epoch 3 Batch 600 Loss 0.1930\n",
            "Epoch 3 Batch 700 Loss 0.1949\n",
            "Epoch 3 Batch 800 Loss 0.2195\n",
            "Epoch 3 Batch 900 Loss 0.1577\n",
            "Epoch 3 Batch 1000 Loss 0.1966\n",
            "Epoch 3 Batch 1100 Loss 0.1667\n",
            "Epoch 3 Batch 1200 Loss 0.1799\n",
            "Epoch 3 Batch 1300 Loss 0.2100\n",
            "Epoch 3 Batch 1400 Loss 0.2009\n",
            "Epoch 3 Loss 0.1924\n",
            "Time taken for 1 epoch 268.98413848876953 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.1745\n",
            "Epoch 4 Batch 100 Loss 0.2262\n",
            "Epoch 4 Batch 200 Loss 0.1815\n",
            "Epoch 4 Batch 300 Loss 0.1786\n",
            "Epoch 4 Batch 400 Loss 0.1823\n",
            "Epoch 4 Batch 500 Loss 0.1813\n",
            "Epoch 4 Batch 600 Loss 0.1760\n",
            "Epoch 4 Batch 700 Loss 0.1886\n",
            "Epoch 4 Batch 800 Loss 0.1920\n",
            "Epoch 4 Batch 900 Loss 0.2130\n",
            "Epoch 4 Batch 1000 Loss 0.1924\n",
            "Epoch 4 Batch 1100 Loss 0.2028\n",
            "Epoch 4 Batch 1200 Loss 0.2060\n",
            "Epoch 4 Batch 1300 Loss 0.2331\n",
            "Epoch 4 Batch 1400 Loss 0.1902\n",
            "Epoch 4 Loss 0.1797\n",
            "Time taken for 1 epoch 269.1371216773987 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1864\n",
            "Epoch 5 Batch 100 Loss 0.1700\n",
            "Epoch 5 Batch 200 Loss 0.1632\n",
            "Epoch 5 Batch 300 Loss 0.1468\n",
            "Epoch 5 Batch 400 Loss 0.1298\n",
            "Epoch 5 Batch 500 Loss 0.1736\n",
            "Epoch 5 Batch 600 Loss 0.1953\n",
            "Epoch 5 Batch 700 Loss 0.1583\n",
            "Epoch 5 Batch 800 Loss 0.1780\n",
            "Epoch 5 Batch 900 Loss 0.1891\n",
            "Epoch 5 Batch 1000 Loss 0.1664\n",
            "Epoch 5 Batch 1100 Loss 0.1490\n",
            "Epoch 5 Batch 1200 Loss 0.1597\n",
            "Epoch 5 Batch 1300 Loss 0.1697\n",
            "Epoch 5 Batch 1400 Loss 0.1560\n",
            "Epoch 5 Loss 0.1702\n",
            "Time taken for 1 epoch 268.93397545814514 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1687\n",
            "Epoch 6 Batch 100 Loss 0.1337\n",
            "Epoch 6 Batch 200 Loss 0.1425\n",
            "Epoch 6 Batch 300 Loss 0.1427\n",
            "Epoch 6 Batch 400 Loss 0.1499\n",
            "Epoch 6 Batch 500 Loss 0.1604\n",
            "Epoch 6 Batch 600 Loss 0.1615\n",
            "Epoch 6 Batch 700 Loss 0.1526\n",
            "Epoch 6 Batch 800 Loss 0.1507\n",
            "Epoch 6 Batch 900 Loss 0.1609\n",
            "Epoch 6 Batch 1000 Loss 0.1690\n",
            "Epoch 6 Batch 1100 Loss 0.1713\n",
            "Epoch 6 Batch 1200 Loss 0.1474\n",
            "Epoch 6 Batch 1300 Loss 0.1473\n",
            "Epoch 6 Batch 1400 Loss 0.1323\n",
            "Epoch 6 Loss 0.1583\n",
            "Time taken for 1 epoch 269.0506544113159 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1138\n",
            "Epoch 7 Batch 100 Loss 0.1650\n",
            "Epoch 7 Batch 200 Loss 0.1222\n",
            "Epoch 7 Batch 300 Loss 0.1541\n",
            "Epoch 7 Batch 400 Loss 0.1604\n",
            "Epoch 7 Batch 500 Loss 0.1427\n",
            "Epoch 7 Batch 600 Loss 0.1464\n",
            "Epoch 7 Batch 700 Loss 0.1465\n",
            "Epoch 7 Batch 800 Loss 0.1358\n",
            "Epoch 7 Batch 900 Loss 0.1234\n",
            "Epoch 7 Batch 1000 Loss 0.1747\n",
            "Epoch 7 Batch 1100 Loss 0.1249\n",
            "Epoch 7 Batch 1200 Loss 0.1489\n",
            "Epoch 7 Batch 1300 Loss 0.1689\n",
            "Epoch 7 Batch 1400 Loss 0.1346\n",
            "Epoch 7 Loss 0.1459\n",
            "Time taken for 1 epoch 268.85655403137207 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1320\n",
            "Epoch 8 Batch 100 Loss 0.1292\n",
            "Epoch 8 Batch 200 Loss 0.1308\n",
            "Epoch 8 Batch 300 Loss 0.1512\n",
            "Epoch 8 Batch 400 Loss 0.1392\n",
            "Epoch 8 Batch 500 Loss 0.1530\n",
            "Epoch 8 Batch 600 Loss 0.1233\n",
            "Epoch 8 Batch 700 Loss 0.1225\n",
            "Epoch 8 Batch 800 Loss 0.1093\n",
            "Epoch 8 Batch 900 Loss 0.1387\n",
            "Epoch 8 Batch 1000 Loss 0.1413\n",
            "Epoch 8 Batch 1100 Loss 0.1658\n",
            "Epoch 8 Batch 1200 Loss 0.1406\n",
            "Epoch 8 Batch 1300 Loss 0.1313\n",
            "Epoch 8 Batch 1400 Loss 0.1221\n",
            "Epoch 8 Loss 0.1353\n",
            "Time taken for 1 epoch 269.04227232933044 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.1112\n",
            "Epoch 9 Batch 100 Loss 0.1086\n",
            "Epoch 9 Batch 200 Loss 0.0958\n",
            "Epoch 9 Batch 300 Loss 0.1465\n",
            "Epoch 9 Batch 400 Loss 0.0911\n",
            "Epoch 9 Batch 500 Loss 0.1326\n",
            "Epoch 9 Batch 600 Loss 0.1224\n",
            "Epoch 9 Batch 700 Loss 0.1158\n",
            "Epoch 9 Batch 800 Loss 0.1244\n",
            "Epoch 9 Batch 900 Loss 0.1265\n",
            "Epoch 9 Batch 1000 Loss 0.1220\n",
            "Epoch 9 Batch 1100 Loss 0.1299\n",
            "Epoch 9 Batch 1200 Loss 0.1271\n",
            "Epoch 9 Batch 1300 Loss 0.1411\n",
            "Epoch 9 Batch 1400 Loss 0.1373\n",
            "Epoch 9 Loss 0.1235\n",
            "Time taken for 1 epoch 268.84223461151123 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.1041\n",
            "Epoch 10 Batch 100 Loss 0.1111\n",
            "Epoch 10 Batch 200 Loss 0.1086\n",
            "Epoch 10 Batch 300 Loss 0.0927\n",
            "Epoch 10 Batch 400 Loss 0.0981\n",
            "Epoch 10 Batch 500 Loss 0.1093\n",
            "Epoch 10 Batch 600 Loss 0.1171\n",
            "Epoch 10 Batch 700 Loss 0.1067\n",
            "Epoch 10 Batch 800 Loss 0.0992\n",
            "Epoch 10 Batch 900 Loss 0.1178\n",
            "Epoch 10 Batch 1000 Loss 0.1180\n",
            "Epoch 10 Batch 1100 Loss 0.1153\n",
            "Epoch 10 Batch 1200 Loss 0.1364\n",
            "Epoch 10 Batch 1300 Loss 0.1049\n",
            "Epoch 10 Batch 1400 Loss 0.1120\n",
            "Epoch 10 Loss 0.1140\n",
            "Time taken for 1 epoch 269.08294224739075 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Transliterate\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "* And store the *attention weights for every time step*.\n",
        "\n",
        "Note: The encoder output is calculated only once for one input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sl9zUHzg3jGI",
        "colab": {}
      },
      "source": [
        "arabic_diacritics = re.compile(\n",
        "    \"\"\"\n",
        "                             ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\",\n",
        "    re.VERBOSE,\n",
        ")\n",
        "\n",
        "arabic_punctuations = \"\"\"`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ\"\"\"\n",
        "english_punctuations = string.punctuation\n",
        "punctuations_list = arabic_punctuations + english_punctuations\n",
        "translator = str.maketrans(\"\", \"\", punctuations_list)\n",
        "unicode_chars = [\"\\u200c\", \"\\u200e\", \"\\u200f\", \"\\u202c\"]\n",
        "\n",
        "\n",
        "def remove_problematic_unicode(text: str) -> str:\n",
        "    for x in unicode_chars:\n",
        "        text = text.replace(x, \"\")\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def normalize_arabic(text: str) -> str:\n",
        "    text = re.sub(\"[إأآاٱ]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_diacritics(text: str) -> str:\n",
        "    text = re.sub(arabic_diacritics, \"\", text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def prepare_name(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    text = text.translate(translator)\n",
        "    text = remove_diacritics(text)\n",
        "    text = normalize_arabic(text)\n",
        "    text = remove_problematic_unicode(text)\n",
        "    return [\"!\" + e + \"?\" for e in text.split(\" \")]\n",
        "\n",
        "\n",
        "def evaluate(sentence):\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        [inputs], maxlen=max_length_inp, padding=\"post\"\n",
        "    )\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = \"\"\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index[\"!\"]], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(\n",
        "            dec_input, dec_hidden, enc_out\n",
        "        )\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + \" \"\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == \"$\":\n",
        "            return result, sentence\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    result = result.split(\"?\")[0].replace(\" \", \"\")\n",
        "\n",
        "    return result, sentence\n",
        "\n",
        "\n",
        "def transliterate(text: str) -> str:\n",
        "    names = prepare_name(text)\n",
        "    result = \" \".join([evaluate(name)[0] for name in names])\n",
        "\n",
        "    print(\"Input: %s\" % (text))\n",
        "    print(\"Predicted translation: {}\".format(result))\n",
        "\n",
        "    return result\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UJpT9D5_OgP6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c60693a0-6a89-4f83-bba4-18d1a0dcfc15"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb9d0f776d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbiwHvV0tDf6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d4744b5-2411-4771-ddab-f6dc427241d9"
      },
      "source": [
        "names = {\n",
        "    \"Mohammed\": \"محمد‎\",\n",
        "    \"Mamun\": \"مامون\",\n",
        "    \"Urdu\": \"فیضان‎\",\n",
        "    \"Thomas\": \"توماس\",\n",
        "    \"Léna\": \"لينا\",\n",
        "    \"Jean\": \"جينز\",\n",
        "    \"Boubacar\": \"بوبكر\",\n",
        "    \"Ghita\": \"غيتا\",\n",
        "    \"Ezékiel\": \"حزقيال\",\n",
        "    \"Gaspard\": \"جاسبارد\",\n",
        "    \"Balthasar\": \"بالتازار\",\n",
        "    \"Olivier\": \"أوليفر\",\n",
        "    \"Jason\": \"جيسون\",\n",
        "    \"Nicolas\": \"نيكولاس\",\n",
        "    \"George\": \"جورج\",\n",
        "    \"Joséphine\": \"جوزفين\",\n",
        "    \"Cunégonde\": \"كونيجوند\",\n",
        "    \"Hortense\": \"هورتنس\",\n",
        "}\n",
        "\n",
        "for latin, arabic in names.items():\n",
        "    transliterate(arabic)\n",
        "    print(f\"Ground truth         : {latin}\")\n",
        "    print(\"-------\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: محمد‎\n",
            "Predicted translation: mohammad\n",
            "Ground truth         : Mohammed\n",
            "-------\n",
            "Input: مامون\n",
            "Predicted translation: mamon\n",
            "Ground truth         : Mamun\n",
            "-------\n",
            "Input: فیضان‎\n",
            "Predicted translation: faizan\n",
            "Ground truth         : Urdu\n",
            "-------\n",
            "Input: توماس\n",
            "Predicted translation: tomas\n",
            "Ground truth         : Thomas\n",
            "-------\n",
            "Input: لينا\n",
            "Predicted translation: lenna\n",
            "Ground truth         : Léna\n",
            "-------\n",
            "Input: جينز\n",
            "Predicted translation: jenz\n",
            "Ground truth         : Jean\n",
            "-------\n",
            "Input: بوبكر\n",
            "Predicted translation: bobaker\n",
            "Ground truth         : Boubacar\n",
            "-------\n",
            "Input: غيتا\n",
            "Predicted translation: gita\n",
            "Ground truth         : Ghita\n",
            "-------\n",
            "Input: حزقيال\n",
            "Predicted translation: hizqial\n",
            "Ground truth         : Ezékiel\n",
            "-------\n",
            "Input: جاسبارد\n",
            "Predicted translation: jasbard\n",
            "Ground truth         : Gaspard\n",
            "-------\n",
            "Input: بالتازار\n",
            "Predicted translation: baltazar\n",
            "Ground truth         : Balthasar\n",
            "-------\n",
            "Input: أوليفر\n",
            "Predicted translation: oliver\n",
            "Ground truth         : Olivier\n",
            "-------\n",
            "Input: جيسون\n",
            "Predicted translation: jesson\n",
            "Ground truth         : Jason\n",
            "-------\n",
            "Input: نيكولاس\n",
            "Predicted translation: nicolas\n",
            "Ground truth         : Nicolas\n",
            "-------\n",
            "Input: جورج\n",
            "Predicted translation: gorg\n",
            "Ground truth         : George\n",
            "-------\n",
            "Input: جوزفين\n",
            "Predicted translation: josephin\n",
            "Ground truth         : Joséphine\n",
            "-------\n",
            "Input: كونيجوند\n",
            "Predicted translation: conijunde\n",
            "Ground truth         : Cunégonde\n",
            "-------\n",
            "Input: هورتنس\n",
            "Predicted translation: hortens\n",
            "Ground truth         : Hortense\n",
            "-------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMFlALytGvEU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "07c9dc78-d83d-4ede-bff1-8408ca1678f6"
      },
      "source": [
        "famous_arabs = {\n",
        "    \"Boutros Boutros-Ghali\": \"بطرس بطرس غالي\",\n",
        "    \"Rifa'a al-Tahtawi\": \"رفاعة رافع الطهطاوي\",\n",
        "    \"Saad Zaghloul\": \"سعد زغلول‎\",\n",
        "    \"Farouk El-Baz\": \"فاروق الباز‎\",\n",
        "    \"Abū ʿAbdallāh Yaʿīsh ibn Ibrāhīm ibn Yūsuf ibn Simāk al-Andalusī al-Umawī\": \"يعيش بن إبراهيم بن يوسف بن سماك الأموي الأندلسي\",\n",
        "    \"Ahmed Hassan Zewail\": \"أحمد حسن زويل‎\",\n",
        "    \"Abdel-Wahed El-Wakil\": \"عبد الواحد الوكيل‎\",\n",
        "    \"Suad Amiry\": \"سعاد العامري‎\",\n",
        "    \"Aḥmad ibn Faḍlān ibn al-ʿAbbās ibn Rāšid ibn Ḥammād\": \"أحمد بن فضلان بن العباس بن راشد بن حماد‎\",\n",
        "    \"Ahmad ibn Mājid\": \"أحمد بن ماجد\",\n",
        "    \"Abbas Mahmoud al-Aqqad\": \"عباس محمود العقاد‎\",\n",
        "    \"Imru' al-Qais Junduh bin Hujr al-Kindi \": \"ٱمْرُؤ ٱلْقَيْس جُنْدُح ٱبْن حُجْر ٱلْكِنْدِيّ‎\",\n",
        "    \"Abū al-Qāsim Khalaf ibn al-'Abbās al-Zahrāwī al-Ansari\": \"أبو القاسم خلف بن العباس الزهراوي\",\n",
        "}\n",
        "\n",
        "for latin, arabic in famous_arabs.items():\n",
        "    transliterate(arabic)\n",
        "    print(f\"Ground truth         : {latin}\")\n",
        "    print(\"-------\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: بطرس بطرس غالي\n",
            "Predicted translation: botros botros galli\n",
            "Ground truth         : Boutros Boutros-Ghali\n",
            "-------\n",
            "Input: رفاعة رافع الطهطاوي\n",
            "Predicted translation: refaa rafaa tahatawi\n",
            "Ground truth         : Rifa'a al-Tahtawi\n",
            "-------\n",
            "Input: سعد زغلول‎\n",
            "Predicted translation: saad zaghloul\n",
            "Ground truth         : Saad Zaghloul\n",
            "-------\n",
            "Input: فاروق الباز‎\n",
            "Predicted translation: farouq albaz\n",
            "Ground truth         : Farouk El-Baz\n",
            "-------\n",
            "Input: يعيش بن إبراهيم بن يوسف بن سماك الأموي الأندلسي\n",
            "Predicted translation: yaish ben abrahim ben yousuf ben smak alamoui andolsi\n",
            "Ground truth         : Abū ʿAbdallāh Yaʿīsh ibn Ibrāhīm ibn Yūsuf ibn Simāk al-Andalusī al-Umawī\n",
            "-------\n",
            "Input: أحمد حسن زويل‎\n",
            "Predicted translation: ahmed hassan zoel\n",
            "Ground truth         : Ahmed Hassan Zewail\n",
            "-------\n",
            "Input: عبد الواحد الوكيل‎\n",
            "Predicted translation: abder waahid alokil\n",
            "Ground truth         : Abdel-Wahed El-Wakil\n",
            "-------\n",
            "Input: سعاد العامري‎\n",
            "Predicted translation: suad amiri\n",
            "Ground truth         : Suad Amiry\n",
            "-------\n",
            "Input: أحمد بن فضلان بن العباس بن راشد بن حماد‎\n",
            "Predicted translation: ahmed ben fadlan ben abbas ben rachid ben hamad\n",
            "Ground truth         : Aḥmad ibn Faḍlān ibn al-ʿAbbās ibn Rāšid ibn Ḥammād\n",
            "-------\n",
            "Input: أحمد بن ماجد\n",
            "Predicted translation: ahmed ben majed\n",
            "Ground truth         : Ahmad ibn Mājid\n",
            "-------\n",
            "Input: عباس محمود العقاد‎\n",
            "Predicted translation: abbas mahmood akkad\n",
            "Ground truth         : Abbas Mahmoud al-Aqqad\n",
            "-------\n",
            "Input: ٱمْرُؤ ٱلْقَيْس جُنْدُح ٱبْن حُجْر ٱلْكِنْدِيّ‎\n",
            "Predicted translation: amree alkis jandah aben hajr kandi\n",
            "Ground truth         : Imru' al-Qais Junduh bin Hujr al-Kindi \n",
            "-------\n",
            "Input: أبو القاسم خلف بن العباس الزهراوي\n",
            "Predicted translation: abouel qasim khalaf ben abbas zahraoui\n",
            "Ground truth         : Abū al-Qāsim Khalaf ibn al-'Abbās al-Zahrāwī al-Ansari\n",
            "-------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOCeLhuyhzfF",
        "colab_type": "text"
      },
      "source": [
        "## Store everything to reproduce results elsewhere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y6b9RVHh5Tc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_keras_tokenizer_json(tokenizer, path):\n",
        "  # save keras tokenizer for input language (arabic)\n",
        "  tokenizer_json = tokenizer.to_json()\n",
        "  with io.open(path, 'w', encoding='utf-8') as f:\n",
        "      f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
        "\n",
        "def load_keras_tokenizer_json(path):\n",
        "  # load keras tokenizer for input language (arabic)\n",
        "  with open(path) as f:\n",
        "      data = json.load(f)\n",
        "      tokenizer = tokenizer_from_json(data)\n",
        "  return tokenizer\n",
        "\n",
        "def save_metadata(metadata, path):\n",
        "  with io.open(path, 'w', encoding='utf-8') as f:\n",
        "      f.write(json.dumps(metadata, ensure_ascii=False))\n",
        "\n",
        "def load_metadata(path):\n",
        "  # load keras tokenizer for input language (arabic)\n",
        "  with open(path) as f:\n",
        "      data = json.load(f)\n",
        "  return data\n",
        "\n",
        "\n",
        "metadata = {\n",
        "    \"BATCH_SIZE\": 64,\n",
        "    \"embedding_dim\": 256,\n",
        "    \"units\": 1024,\n",
        "    \"vocab_inp_size\": len(inp_lang.word_index)+1,\n",
        "    \"vocab_tar_size\": len(targ_lang.word_index)+1,\n",
        "    \"max_length_inp\" : max_length_inp\n",
        "}\n",
        "\n",
        "\n",
        "# https://colab.research.google.com/drive/172D4jishSgE3N7AO6U2OKAA_0wNnrMOq#scrollTo=gMg87Tz01cxQ\n",
        "encoder.save_weights(\"./model/encoder/checkpoint\", save_format='tf')\n",
        "decoder.save_weights(\"./model/decoder/checkpoint\", save_format='tf')\n",
        "\n",
        "save_keras_tokenizer_json(inp_lang, \"./model/input_tokenizer.json\")\n",
        "save_keras_tokenizer_json(targ_lang, \"./model/output_tokenizer.json\")\n",
        "\n",
        "save_metadata(metadata, \"./model/ar2en_metadata.json\")\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K7ri9IZAV0F",
        "colab_type": "text"
      },
      "source": [
        "## Recreate from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhOa0UWzhDWd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b20dd1d5-d97c-4539-fd89-92e66aced5bf"
      },
      "source": [
        "# Check loading is working\n",
        "input_tokenizer = load_keras_tokenizer_json(\"./model/input_tokenizer.json\")\n",
        "output_tokenizer = load_keras_tokenizer_json(\"./model/output_tokenizer.json\")\n",
        "loaded_metadata = load_metadata(\"./model/ar2en_metadata.json\")\n",
        "new_encoder, _, new_decoder = get_model(\n",
        "    loaded_metadata[\"vocab_inp_size\"],\n",
        "    loaded_metadata[\"vocab_tar_size\"],\n",
        "    loaded_metadata[\"embedding_dim\"],\n",
        "    loaded_metadata[\"units\"],\n",
        "    loaded_metadata[\"BATCH_SIZE\"],\n",
        ")\n",
        "new_encoder.load_weights(\"./model/encoder/checkpoint\")\n",
        "new_decoder.load_weights(\"./model/decoder/checkpoint\")\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fb962771e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdbZZdiHAyUj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7f0d97a9-3a47-48b1-a412-145c19c84d61"
      },
      "source": [
        "def evaluate(sentence, input_tokenizer, output_tokenizer, encoder, decoder, metadata):\n",
        "\n",
        "    inputs = [input_tokenizer.word_index[i] for i in sentence]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        [inputs], maxlen=metadata[\"max_length_inp\"], padding=\"post\"\n",
        "    )\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = \"\"\n",
        "\n",
        "    hidden = [tf.zeros((1, metadata[\"units\"]))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([output_tokenizer.word_index[\"!\"]], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(\n",
        "            dec_input, dec_hidden, enc_out\n",
        "        )\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += output_tokenizer.index_word[predicted_id] + \" \"\n",
        "\n",
        "        if output_tokenizer.index_word[predicted_id] == \"$\":\n",
        "            return result, sentence\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    result = result.split(\"?\")[0].replace(\" \", \"\")\n",
        "\n",
        "    return result, sentence\n",
        "\n",
        "\n",
        "def transliterate(text: str) -> str:\n",
        "    names = prepare_name(text)\n",
        "    result = \" \".join([evaluate(name, input_tokenizer, output_tokenizer, encoder, decoder, metadata)[0] for name in names])\n",
        "\n",
        "    print(\"Input: %s\" % (text))\n",
        "    print(\"Predicted translation: {}\".format(result))\n",
        "\n",
        "    return result\n",
        "\n",
        "transliterate(\"محمد‎\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: محمد‎\n",
            "Predicted translation: mohammad\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'mohammad'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "arabic_to_english_names_transliteration_with_nmt_and_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomas-chauvet/names_transliteration/blob/master/arabic_to_english_names_transliteration_with_nmt_and_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Names transliteration - Neural machine translation with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CiwtNgENbx2g"
      },
      "source": [
        "This notebook trains a sequence to sequence (seq2seq) model to translitate names with arabic characters to names in latin character. Usually we call this task \"romanization\". It is the task to transform string from one alphabet to latin alphanet.\n",
        "\n",
        "After training the model in this notebook, you will be able to input an arabic name, such as *محمد‎*, and return the transliteration/translation of this name: *mohammad*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuWAqgLEw8zX",
        "colab_type": "text"
      },
      "source": [
        "## Clone repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POgIZ7lWt-Jf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "96a8621f-2983-4119-c3f7-7f81046905f1"
      },
      "source": [
        "! git clone https://github.com/thomas-chauvet/names_transliteration.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'names_transliteration' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ns2feAxw-59",
        "colab_type": "text"
      },
      "source": [
        "## Install python library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RPyxLNSsiKN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33baf5c4-485a-4f52-98da-b7f0cd709ce3"
      },
      "source": [
        "!python names_transliteration/setup.py install"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing names_transliteration.egg-info/PKG-INFO\n",
            "writing dependency_links to names_transliteration.egg-info/dependency_links.txt\n",
            "writing entry points to names_transliteration.egg-info/entry_points.txt\n",
            "writing requirements to names_transliteration.egg-info/requires.txt\n",
            "writing top-level names to names_transliteration.egg-info/top_level.txt\n",
            "reading manifest file 'names_transliteration.egg-info/SOURCES.txt'\n",
            "writing manifest file 'names_transliteration.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n",
            "\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying names_transliteration.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying names_transliteration.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying names_transliteration.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying names_transliteration.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying names_transliteration.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying names_transliteration.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating 'dist/names_transliteration-0.0.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing names_transliteration-0.0.0-py3.6.egg\n",
            "Removing /usr/local/lib/python3.6/dist-packages/names_transliteration-0.0.0-py3.6.egg\n",
            "Copying names_transliteration-0.0.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "names-transliteration 0.0.0 is already the active version in easy-install.pth\n",
            "Installing names-transliteration-get-data script to /usr/local/bin\n",
            "Installing names-transliteration-train script to /usr/local/bin\n",
            "Installing names-transliteration-transliterate script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/names_transliteration-0.0.0-py3.6.egg\n",
            "Processing dependencies for names-transliteration==0.0.0\n",
            "Searching for pandas==1.1.0\n",
            "Best match: pandas 1.1.0\n",
            "Adding pandas 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tensorflow==2.1.0\n",
            "Best match: tensorflow 2.1.0\n",
            "Adding tensorflow 2.1.0 to easy-install.pth file\n",
            "Installing estimator_ckpt_converter script to /usr/local/bin\n",
            "Installing saved_model_cli script to /usr/local/bin\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "Installing tf_upgrade_v2 script to /usr/local/bin\n",
            "Installing tflite_convert script to /usr/local/bin\n",
            "Installing toco script to /usr/local/bin\n",
            "Installing toco_from_protos script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.18.5\n",
            "Best match: numpy 1.18.5\n",
            "Adding numpy 1.18.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for python-dateutil==2.8.1\n",
            "Best match: python-dateutil 2.8.1\n",
            "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pytz==2018.9\n",
            "Best match: pytz 2018.9\n",
            "Adding pytz 2018.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for termcolor==1.1.0\n",
            "Best match: termcolor 1.1.0\n",
            "Adding termcolor 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for wheel==0.34.2\n",
            "Best match: wheel 0.34.2\n",
            "Adding wheel 0.34.2 to easy-install.pth file\n",
            "Installing wheel script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for grpcio==1.31.0\n",
            "Best match: grpcio 1.31.0\n",
            "Adding grpcio 1.31.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for absl-py==0.9.0\n",
            "Best match: absl-py 0.9.0\n",
            "Adding absl-py 0.9.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tensorboard==2.1.1\n",
            "Best match: tensorboard 2.1.1\n",
            "Adding tensorboard 2.1.1 to easy-install.pth file\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for gast==0.2.2\n",
            "Best match: gast 0.2.2\n",
            "Adding gast 0.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for astor==0.8.1\n",
            "Best match: astor 0.8.1\n",
            "Adding astor 0.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for google-pasta==0.2.0\n",
            "Best match: google-pasta 0.2.0\n",
            "Adding google-pasta 0.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras-Applications==1.0.8\n",
            "Best match: Keras-Applications 1.0.8\n",
            "Adding Keras-Applications 1.0.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for wrapt==1.12.1\n",
            "Best match: wrapt 1.12.1\n",
            "Adding wrapt 1.12.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for opt-einsum==3.3.0\n",
            "Best match: opt-einsum 3.3.0\n",
            "Adding opt-einsum 3.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tensorflow-estimator==2.1.0\n",
            "Best match: tensorflow-estimator 2.1.0\n",
            "Adding tensorflow-estimator 2.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for protobuf==3.12.4\n",
            "Best match: protobuf 3.12.4\n",
            "Adding protobuf 3.12.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras-Preprocessing==1.1.2\n",
            "Best match: Keras-Preprocessing 1.1.2\n",
            "Adding Keras-Preprocessing 1.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for requests==2.23.0\n",
            "Best match: requests 2.23.0\n",
            "Adding requests 2.23.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for setuptools==49.2.0\n",
            "Best match: setuptools 49.2.0\n",
            "Adding setuptools 49.2.0 to easy-install.pth file\n",
            "Installing easy_install script to /usr/local/bin\n",
            "Installing easy_install-3.8 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for google-auth==1.17.2\n",
            "Best match: google-auth 1.17.2\n",
            "Adding google-auth 1.17.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Werkzeug==1.0.1\n",
            "Best match: Werkzeug 1.0.1\n",
            "Adding Werkzeug 1.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Markdown==3.2.2\n",
            "Best match: Markdown 3.2.2\n",
            "Adding Markdown 3.2.2 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for google-auth-oauthlib==0.4.1\n",
            "Best match: google-auth-oauthlib 0.4.1\n",
            "Adding google-auth-oauthlib 0.4.1 to easy-install.pth file\n",
            "Installing google-oauthlib-tool script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for h5py==2.10.0\n",
            "Best match: h5py 2.10.0\n",
            "Adding h5py 2.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for chardet==3.0.4\n",
            "Best match: chardet 3.0.4\n",
            "Adding chardet 3.0.4 to easy-install.pth file\n",
            "Installing chardetect script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for urllib3==1.24.3\n",
            "Best match: urllib3 1.24.3\n",
            "Adding urllib3 1.24.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for idna==2.10\n",
            "Best match: idna 2.10\n",
            "Adding idna 2.10 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for certifi==2020.6.20\n",
            "Best match: certifi 2020.6.20\n",
            "Adding certifi 2020.6.20 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cachetools==4.1.1\n",
            "Best match: cachetools 4.1.1\n",
            "Adding cachetools 4.1.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyasn1-modules==0.2.8\n",
            "Best match: pyasn1-modules 0.2.8\n",
            "Adding pyasn1-modules 0.2.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for rsa==4.6\n",
            "Best match: rsa 4.6\n",
            "Adding rsa 4.6 to easy-install.pth file\n",
            "Installing pyrsa-decrypt script to /usr/local/bin\n",
            "Installing pyrsa-encrypt script to /usr/local/bin\n",
            "Installing pyrsa-keygen script to /usr/local/bin\n",
            "Installing pyrsa-priv2pub script to /usr/local/bin\n",
            "Installing pyrsa-sign script to /usr/local/bin\n",
            "Installing pyrsa-verify script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for importlib-metadata==1.7.0\n",
            "Best match: importlib-metadata 1.7.0\n",
            "Adding importlib-metadata 1.7.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for requests-oauthlib==1.3.0\n",
            "Best match: requests-oauthlib 1.3.0\n",
            "Adding requests-oauthlib 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyasn1==0.4.8\n",
            "Best match: pyasn1 0.4.8\n",
            "Adding pyasn1 0.4.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for zipp==3.1.0\n",
            "Best match: zipp 3.1.0\n",
            "Adding zipp 3.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for oauthlib==3.1.0\n",
            "Best match: oauthlib 3.1.0\n",
            "Adding oauthlib 3.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for names-transliteration==0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8EkZ1oCxBEs",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImaLXZO1thn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transliteration.model.nmt import get_model\n",
        "from transliteration.model.process import load_dataset\n",
        "from transliteration.model.save import save_keras_tokenizer_json, save_model_metadata\n",
        "from transliteration.model.train import train\n",
        "from transliteration.model.save import load_keras_tokenizer_json, load_model_metadata\n",
        "from transliteration.model.transliterate import transliterate\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "logging.basicConfig()\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kER_sgpxDOA",
        "colab_type": "text"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tEKC7vhuk54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_examples = None\n",
        "test_size = 0.2\n",
        "batch_size = 64\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "epochs = 20\n",
        "\n",
        "model_path = Path(\"names_transliteration/model/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0qRetNbxIqW",
        "colab_type": "text"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8eea199e-8e58-41ef-ede0-9b8ea0a15920"
      },
      "source": [
        "logger.info(\"Load dataset\")\n",
        "source_tensor, target_tensor, source, target = load_dataset(\n",
        "    Path(\"names_transliteration/data/clean/arabic_english.csv\"), num_examples=num_examples\n",
        ")\n",
        "\n",
        "logger.info(\"Creating training and validation sets\")\n",
        "(\n",
        "    source_tensor_train,\n",
        "    source_tensor_val,\n",
        "    target_tensor_train,\n",
        "    target_tensor_val,\n",
        ") = train_test_split(source_tensor, target_tensor, test_size=test_size)\n",
        "\n",
        "BUFFER_SIZE = len(source_tensor_train)\n",
        "logger.info(f\"BUFFER_SIZE: {BUFFER_SIZE}\")\n",
        "STEPS_PER_EPOCH = len(source_tensor_train) // batch_size\n",
        "logger.info(f\"STEPS_PER_EPOCH: {STEPS_PER_EPOCH}\")\n",
        "VOCAB_INP_SIZE = len(source.word_index) + 1\n",
        "logger.info(f\"VOCAB_INP_SIZE: {VOCAB_INP_SIZE}\")\n",
        "VOCAB_TAR_SIZE = len(target.word_index) + 1\n",
        "logger.info(f\"VOCAB_TAR_SIZE: {VOCAB_TAR_SIZE}\")\n",
        "\n",
        "logger.info(\"Tensorflow dataset batch\")\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (source_tensor_train, target_tensor_train)\n",
        ").shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "logger.info(\"Instanciate encoder, attention and decoder\")\n",
        "encoder, attention_layer, decoder = get_model(\n",
        "    VOCAB_INP_SIZE, VOCAB_TAR_SIZE, embedding_dim=embedding_dim, units=units, batch_sz=batch_size\n",
        ")\n",
        "\n",
        "logger.info(\"Train model\")\n",
        "encoder, decoder = train(\n",
        "    dataset,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    target,\n",
        "    STEPS_PER_EPOCH,\n",
        "    epochs=epochs,\n",
        "    checkpoint_dir=model_path / \"training_checkpoints\",\n",
        ")\n",
        "\n",
        "metadata = {\n",
        "    \"batch_size\": batch_size,\n",
        "    \"embedding_dim\": embedding_dim,\n",
        "    \"units\": units,\n",
        "    \"vocab_inp_size\": len(source.word_index) + 1,\n",
        "    \"vocab_tar_size\": len(target.word_index) + 1,\n",
        "    \"max_length_source\": source_tensor.shape[1],\n",
        "    \"max_length_target\": target_tensor.shape[1],\n",
        "}\n",
        "\n",
        "logger.info(\"Save models weights\")\n",
        "encoder.save_weights(\n",
        "    (model_path / \"encoder/checkpoint\").as_posix(), save_format=\"tf\"\n",
        ")\n",
        "decoder.save_weights(\n",
        "    (model_path / \"decoder/checkpoint\").as_posix(), save_format=\"tf\"\n",
        ")\n",
        "\n",
        "logger.info(\"Save tokenizers\")\n",
        "save_keras_tokenizer_json(source, model_path / \"source_tokenizer.json\")\n",
        "save_keras_tokenizer_json(target, model_path / \"target_tokenizer.json\")\n",
        "\n",
        "logger.info(\"Save model's metadata\")\n",
        "save_model_metadata(metadata, model_path / \"model_metadata.json\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Load dataset\n",
            "INFO:__main__:Creating training and validation sets\n",
            "INFO:__main__:BUFFER_SIZE: 94438\n",
            "INFO:__main__:STEPS_PER_EPOCH: 1475\n",
            "INFO:__main__:VOCAB_INP_SIZE: 55\n",
            "INFO:__main__:VOCAB_TAR_SIZE: 60\n",
            "INFO:__main__:Tensorflow dataset batch\n",
            "INFO:__main__:Instanciate encoder, attention and decoder\n",
            "INFO:__main__:Train model\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.1517\n",
            "Epoch 1 Batch 100 Loss 0.7790\n",
            "Epoch 1 Batch 200 Loss 0.5619\n",
            "Epoch 1 Batch 300 Loss 0.2848\n",
            "Epoch 1 Batch 400 Loss 0.2902\n",
            "Epoch 1 Batch 500 Loss 0.2274\n",
            "Epoch 1 Batch 600 Loss 0.2146\n",
            "Epoch 1 Batch 700 Loss 0.2315\n",
            "Epoch 1 Batch 800 Loss 0.2603\n",
            "Epoch 1 Batch 900 Loss 0.2534\n",
            "Epoch 1 Batch 1000 Loss 0.2555\n",
            "Epoch 1 Batch 1100 Loss 0.1834\n",
            "Epoch 1 Batch 1200 Loss 0.2418\n",
            "Epoch 1 Batch 1300 Loss 0.2644\n",
            "Epoch 1 Batch 1400 Loss 0.2535\n",
            "Epoch 2 Batch 0 Loss 0.2912\n",
            "Epoch 2 Batch 100 Loss 0.2091\n",
            "Epoch 2 Batch 200 Loss 0.2039\n",
            "Epoch 2 Batch 300 Loss 0.2240\n",
            "Epoch 2 Batch 400 Loss 0.2217\n",
            "Epoch 2 Batch 500 Loss 0.1594\n",
            "Epoch 2 Batch 600 Loss 0.2386\n",
            "Epoch 2 Batch 700 Loss 0.2581\n",
            "Epoch 2 Batch 800 Loss 0.2658\n",
            "Epoch 2 Batch 900 Loss 0.1931\n",
            "Epoch 2 Batch 1000 Loss 0.1971\n",
            "Epoch 2 Batch 1100 Loss 0.1719\n",
            "Epoch 2 Batch 1200 Loss 0.2275\n",
            "Epoch 2 Batch 1300 Loss 0.1963\n",
            "Epoch 2 Batch 1400 Loss 0.1784\n",
            "Epoch 3 Batch 0 Loss 0.1985\n",
            "Epoch 3 Batch 100 Loss 0.1895\n",
            "Epoch 3 Batch 200 Loss 0.2006\n",
            "Epoch 3 Batch 300 Loss 0.1758\n",
            "Epoch 3 Batch 400 Loss 0.1990\n",
            "Epoch 3 Batch 500 Loss 0.1937\n",
            "Epoch 3 Batch 600 Loss 0.2031\n",
            "Epoch 3 Batch 700 Loss 0.1927\n",
            "Epoch 3 Batch 800 Loss 0.1966\n",
            "Epoch 3 Batch 900 Loss 0.1816\n",
            "Epoch 3 Batch 1000 Loss 0.1982\n",
            "Epoch 3 Batch 1100 Loss 0.1852\n",
            "Epoch 3 Batch 1200 Loss 0.2227\n",
            "Epoch 3 Batch 1300 Loss 0.1962\n",
            "Epoch 3 Batch 1400 Loss 0.1655\n",
            "Epoch 4 Batch 0 Loss 0.2010\n",
            "Epoch 4 Batch 100 Loss 0.2017\n",
            "Epoch 4 Batch 200 Loss 0.1868\n",
            "Epoch 4 Batch 300 Loss 0.2197\n",
            "Epoch 4 Batch 400 Loss 0.2242\n",
            "Epoch 4 Batch 500 Loss 0.2026\n",
            "Epoch 4 Batch 600 Loss 0.1928\n",
            "Epoch 4 Batch 700 Loss 0.1969\n",
            "Epoch 4 Batch 800 Loss 0.2078\n",
            "Epoch 4 Batch 900 Loss 0.1606\n",
            "Epoch 4 Batch 1000 Loss 0.1782\n",
            "Epoch 4 Batch 1100 Loss 0.1583\n",
            "Epoch 4 Batch 1200 Loss 0.1760\n",
            "Epoch 4 Batch 1300 Loss 0.2031\n",
            "Epoch 4 Batch 1400 Loss 0.1925\n",
            "Epoch 5 Batch 0 Loss 0.1779\n",
            "Epoch 5 Batch 100 Loss 0.1803\n",
            "Epoch 5 Batch 200 Loss 0.2053\n",
            "Epoch 5 Batch 300 Loss 0.1875\n",
            "Epoch 5 Batch 400 Loss 0.1880\n",
            "Epoch 5 Batch 500 Loss 0.1946\n",
            "Epoch 5 Batch 600 Loss 0.1568\n",
            "Epoch 5 Batch 700 Loss 0.1727\n",
            "Epoch 5 Batch 800 Loss 0.1746\n",
            "Epoch 5 Batch 900 Loss 0.1711\n",
            "Epoch 5 Batch 1000 Loss 0.1999\n",
            "Epoch 5 Batch 1100 Loss 0.1516\n",
            "Epoch 5 Batch 1200 Loss 0.1747\n",
            "Epoch 5 Batch 1300 Loss 0.1758\n",
            "Epoch 5 Batch 1400 Loss 0.1628\n",
            "Epoch 6 Batch 0 Loss 0.1383\n",
            "Epoch 6 Batch 100 Loss 0.1642\n",
            "Epoch 6 Batch 200 Loss 0.1572\n",
            "Epoch 6 Batch 300 Loss 0.1706\n",
            "Epoch 6 Batch 400 Loss 0.1787\n",
            "Epoch 6 Batch 500 Loss 0.1537\n",
            "Epoch 6 Batch 600 Loss 0.1595\n",
            "Epoch 6 Batch 700 Loss 0.1630\n",
            "Epoch 6 Batch 800 Loss 0.2119\n",
            "Epoch 6 Batch 900 Loss 0.1420\n",
            "Epoch 6 Batch 1000 Loss 0.1733\n",
            "Epoch 6 Batch 1100 Loss 0.1532\n",
            "Epoch 6 Batch 1200 Loss 0.1533\n",
            "Epoch 6 Batch 1300 Loss 0.1893\n",
            "Epoch 6 Batch 1400 Loss 0.2103\n",
            "Epoch 7 Batch 0 Loss 0.1313\n",
            "Epoch 7 Batch 100 Loss 0.1315\n",
            "Epoch 7 Batch 200 Loss 0.1475\n",
            "Epoch 7 Batch 300 Loss 0.1557\n",
            "Epoch 7 Batch 400 Loss 0.1461\n",
            "Epoch 7 Batch 500 Loss 0.1827\n",
            "Epoch 7 Batch 600 Loss 0.1639\n",
            "Epoch 7 Batch 700 Loss 0.1863\n",
            "Epoch 7 Batch 800 Loss 0.1432\n",
            "Epoch 7 Batch 900 Loss 0.1841\n",
            "Epoch 7 Batch 1000 Loss 0.1531\n",
            "Epoch 7 Batch 1100 Loss 0.1504\n",
            "Epoch 7 Batch 1200 Loss 0.1775\n",
            "Epoch 7 Batch 1300 Loss 0.1661\n",
            "Epoch 7 Batch 1400 Loss 0.1718\n",
            "Epoch 8 Batch 0 Loss 0.1305\n",
            "Epoch 8 Batch 100 Loss 0.1382\n",
            "Epoch 8 Batch 200 Loss 0.1636\n",
            "Epoch 8 Batch 300 Loss 0.1647\n",
            "Epoch 8 Batch 400 Loss 0.1733\n",
            "Epoch 8 Batch 500 Loss 0.1487\n",
            "Epoch 8 Batch 600 Loss 0.1366\n",
            "Epoch 8 Batch 700 Loss 0.1769\n",
            "Epoch 8 Batch 800 Loss 0.1996\n",
            "Epoch 8 Batch 900 Loss 0.1502\n",
            "Epoch 8 Batch 1000 Loss 0.1483\n",
            "Epoch 8 Batch 1100 Loss 0.1665\n",
            "Epoch 8 Batch 1200 Loss 0.1393\n",
            "Epoch 8 Batch 1300 Loss 0.1729\n",
            "Epoch 8 Batch 1400 Loss 0.1732\n",
            "Epoch 9 Batch 0 Loss 0.1309\n",
            "Epoch 9 Batch 100 Loss 0.1459\n",
            "Epoch 9 Batch 200 Loss 0.1388\n",
            "Epoch 9 Batch 300 Loss 0.1216\n",
            "Epoch 9 Batch 400 Loss 0.1709\n",
            "Epoch 9 Batch 500 Loss 0.1486\n",
            "Epoch 9 Batch 600 Loss 0.1410\n",
            "Epoch 9 Batch 700 Loss 0.1370\n",
            "Epoch 9 Batch 800 Loss 0.1509\n",
            "Epoch 9 Batch 900 Loss 0.1301\n",
            "Epoch 9 Batch 1000 Loss 0.1785\n",
            "Epoch 9 Batch 1100 Loss 0.1591\n",
            "Epoch 9 Batch 1200 Loss 0.1328\n",
            "Epoch 9 Batch 1300 Loss 0.1216\n",
            "Epoch 9 Batch 1400 Loss 0.1390\n",
            "Epoch 10 Batch 0 Loss 0.1139\n",
            "Epoch 10 Batch 100 Loss 0.1134\n",
            "Epoch 10 Batch 200 Loss 0.1191\n",
            "Epoch 10 Batch 300 Loss 0.1294\n",
            "Epoch 10 Batch 400 Loss 0.1227\n",
            "Epoch 10 Batch 500 Loss 0.1248\n",
            "Epoch 10 Batch 600 Loss 0.1311\n",
            "Epoch 10 Batch 700 Loss 0.1486\n",
            "Epoch 10 Batch 800 Loss 0.1382\n",
            "Epoch 10 Batch 900 Loss 0.1398\n",
            "Epoch 10 Batch 1000 Loss 0.1345\n",
            "Epoch 10 Batch 1100 Loss 0.1272\n",
            "Epoch 10 Batch 1200 Loss 0.1477\n",
            "Epoch 10 Batch 1300 Loss 0.1433\n",
            "Epoch 10 Batch 1400 Loss 0.1226\n",
            "Epoch 11 Batch 0 Loss 0.1171\n",
            "Epoch 11 Batch 100 Loss 0.1159\n",
            "Epoch 11 Batch 200 Loss 0.1125\n",
            "Epoch 11 Batch 300 Loss 0.1205\n",
            "Epoch 11 Batch 400 Loss 0.1494\n",
            "Epoch 11 Batch 500 Loss 0.0998\n",
            "Epoch 11 Batch 600 Loss 0.1069\n",
            "Epoch 11 Batch 700 Loss 0.1517\n",
            "Epoch 11 Batch 800 Loss 0.1153\n",
            "Epoch 11 Batch 900 Loss 0.1314\n",
            "Epoch 11 Batch 1000 Loss 0.1383\n",
            "Epoch 11 Batch 1100 Loss 0.1100\n",
            "Epoch 11 Batch 1200 Loss 0.1366\n",
            "Epoch 11 Batch 1300 Loss 0.1136\n",
            "Epoch 11 Batch 1400 Loss 0.1269\n",
            "Epoch 12 Batch 0 Loss 0.1252\n",
            "Epoch 12 Batch 100 Loss 0.0922\n",
            "Epoch 12 Batch 200 Loss 0.1365\n",
            "Epoch 12 Batch 300 Loss 0.1202\n",
            "Epoch 12 Batch 400 Loss 0.1096\n",
            "Epoch 12 Batch 500 Loss 0.1311\n",
            "Epoch 12 Batch 600 Loss 0.1127\n",
            "Epoch 12 Batch 700 Loss 0.1477\n",
            "Epoch 12 Batch 800 Loss 0.1273\n",
            "Epoch 12 Batch 900 Loss 0.1087\n",
            "Epoch 12 Batch 1000 Loss 0.1149\n",
            "Epoch 12 Batch 1100 Loss 0.1174\n",
            "Epoch 12 Batch 1200 Loss 0.1240\n",
            "Epoch 12 Batch 1300 Loss 0.1262\n",
            "Epoch 12 Batch 1400 Loss 0.1157\n",
            "Epoch 13 Batch 0 Loss 0.0948\n",
            "Epoch 13 Batch 100 Loss 0.0934\n",
            "Epoch 13 Batch 200 Loss 0.1164\n",
            "Epoch 13 Batch 300 Loss 0.1107\n",
            "Epoch 13 Batch 400 Loss 0.1122\n",
            "Epoch 13 Batch 500 Loss 0.1012\n",
            "Epoch 13 Batch 600 Loss 0.0978\n",
            "Epoch 13 Batch 700 Loss 0.1315\n",
            "Epoch 13 Batch 800 Loss 0.1308\n",
            "Epoch 13 Batch 900 Loss 0.1099\n",
            "Epoch 13 Batch 1000 Loss 0.1186\n",
            "Epoch 13 Batch 1100 Loss 0.1226\n",
            "Epoch 13 Batch 1200 Loss 0.1170\n",
            "Epoch 13 Batch 1300 Loss 0.1034\n",
            "Epoch 13 Batch 1400 Loss 0.1148\n",
            "Epoch 14 Batch 0 Loss 0.1016\n",
            "Epoch 14 Batch 100 Loss 0.0897\n",
            "Epoch 14 Batch 200 Loss 0.1019\n",
            "Epoch 14 Batch 300 Loss 0.1169\n",
            "Epoch 14 Batch 400 Loss 0.1023\n",
            "Epoch 14 Batch 500 Loss 0.0941\n",
            "Epoch 14 Batch 600 Loss 0.1110\n",
            "Epoch 14 Batch 700 Loss 0.1272\n",
            "Epoch 14 Batch 800 Loss 0.1168\n",
            "Epoch 14 Batch 900 Loss 0.0964\n",
            "Epoch 14 Batch 1000 Loss 0.0973\n",
            "Epoch 14 Batch 1100 Loss 0.1082\n",
            "Epoch 14 Batch 1200 Loss 0.0950\n",
            "Epoch 14 Batch 1300 Loss 0.1082\n",
            "Epoch 14 Batch 1400 Loss 0.0987\n",
            "Epoch 15 Batch 0 Loss 0.0992\n",
            "Epoch 15 Batch 100 Loss 0.0946\n",
            "Epoch 15 Batch 200 Loss 0.1162\n",
            "Epoch 15 Batch 300 Loss 0.1021\n",
            "Epoch 15 Batch 400 Loss 0.0927\n",
            "Epoch 15 Batch 500 Loss 0.0775\n",
            "Epoch 15 Batch 600 Loss 0.0868\n",
            "Epoch 15 Batch 700 Loss 0.1127\n",
            "Epoch 15 Batch 800 Loss 0.1390\n",
            "Epoch 15 Batch 900 Loss 0.1203\n",
            "Epoch 15 Batch 1000 Loss 0.1028\n",
            "Epoch 15 Batch 1100 Loss 0.1149\n",
            "Epoch 15 Batch 1200 Loss 0.0856\n",
            "Epoch 15 Batch 1300 Loss 0.1093\n",
            "Epoch 15 Batch 1400 Loss 0.0953\n",
            "Epoch 16 Batch 0 Loss 0.0792\n",
            "Epoch 16 Batch 100 Loss 0.0901\n",
            "Epoch 16 Batch 200 Loss 0.0797\n",
            "Epoch 16 Batch 300 Loss 0.0659\n",
            "Epoch 16 Batch 400 Loss 0.0890\n",
            "Epoch 16 Batch 500 Loss 0.0961\n",
            "Epoch 16 Batch 600 Loss 0.0786\n",
            "Epoch 16 Batch 700 Loss 0.1194\n",
            "Epoch 16 Batch 800 Loss 0.0991\n",
            "Epoch 16 Batch 900 Loss 0.0929\n",
            "Epoch 16 Batch 1000 Loss 0.0988\n",
            "Epoch 16 Batch 1100 Loss 0.0907\n",
            "Epoch 16 Batch 1200 Loss 0.0971\n",
            "Epoch 16 Batch 1300 Loss 0.0987\n",
            "Epoch 16 Batch 1400 Loss 0.1025\n",
            "Epoch 17 Batch 0 Loss 0.0854\n",
            "Epoch 17 Batch 100 Loss 0.0769\n",
            "Epoch 17 Batch 200 Loss 0.0826\n",
            "Epoch 17 Batch 300 Loss 0.0793\n",
            "Epoch 17 Batch 400 Loss 0.0744\n",
            "Epoch 17 Batch 500 Loss 0.0834\n",
            "Epoch 17 Batch 600 Loss 0.0767\n",
            "Epoch 17 Batch 700 Loss 0.0984\n",
            "Epoch 17 Batch 800 Loss 0.1055\n",
            "Epoch 17 Batch 900 Loss 0.1089\n",
            "Epoch 17 Batch 1000 Loss 0.1124\n",
            "Epoch 17 Batch 1100 Loss 0.1006\n",
            "Epoch 17 Batch 1200 Loss 0.0916\n",
            "Epoch 17 Batch 1300 Loss 0.0945\n",
            "Epoch 17 Batch 1400 Loss 0.0880\n",
            "Epoch 18 Batch 0 Loss 0.0748\n",
            "Epoch 18 Batch 100 Loss 0.0572\n",
            "Epoch 18 Batch 200 Loss 0.0931\n",
            "Epoch 18 Batch 300 Loss 0.0919\n",
            "Epoch 18 Batch 400 Loss 0.0753\n",
            "Epoch 18 Batch 500 Loss 0.0826\n",
            "Epoch 18 Batch 600 Loss 0.1155\n",
            "Epoch 18 Batch 700 Loss 0.0701\n",
            "Epoch 18 Batch 800 Loss 0.0757\n",
            "Epoch 18 Batch 900 Loss 0.0978\n",
            "Epoch 18 Batch 1000 Loss 0.0809\n",
            "Epoch 18 Batch 1100 Loss 0.1044\n",
            "Epoch 18 Batch 1200 Loss 0.0927\n",
            "Epoch 18 Batch 1300 Loss 0.0908\n",
            "Epoch 18 Batch 1400 Loss 0.1133\n",
            "Epoch 19 Batch 0 Loss 0.0737\n",
            "Epoch 19 Batch 100 Loss 0.0831\n",
            "Epoch 19 Batch 200 Loss 0.0668\n",
            "Epoch 19 Batch 300 Loss 0.0627\n",
            "Epoch 19 Batch 400 Loss 0.0841\n",
            "Epoch 19 Batch 500 Loss 0.0675\n",
            "Epoch 19 Batch 600 Loss 0.0767\n",
            "Epoch 19 Batch 700 Loss 0.1003\n",
            "Epoch 19 Batch 800 Loss 0.0871\n",
            "Epoch 19 Batch 900 Loss 0.0833\n",
            "Epoch 19 Batch 1000 Loss 0.0790\n",
            "Epoch 19 Batch 1100 Loss 0.0860\n",
            "Epoch 19 Batch 1200 Loss 0.0794\n",
            "Epoch 19 Batch 1300 Loss 0.0692\n",
            "Epoch 19 Batch 1400 Loss 0.0781\n",
            "Epoch 20 Batch 0 Loss 0.0749\n",
            "Epoch 20 Batch 100 Loss 0.0643\n",
            "Epoch 20 Batch 200 Loss 0.0617\n",
            "Epoch 20 Batch 300 Loss 0.0634\n",
            "Epoch 20 Batch 400 Loss 0.0716\n",
            "Epoch 20 Batch 500 Loss 0.0739\n",
            "Epoch 20 Batch 600 Loss 0.0819\n",
            "Epoch 20 Batch 700 Loss 0.0825\n",
            "Epoch 20 Batch 800 Loss 0.0667\n",
            "Epoch 20 Batch 900 Loss 0.0742\n",
            "Epoch 20 Batch 1000 Loss 0.0811\n",
            "Epoch 20 Batch 1100 Loss 0.0786\n",
            "Epoch 20 Batch 1200 Loss 0.0770\n",
            "Epoch 20 Batch 1300 Loss 0.0919\n",
            "Epoch 20 Batch 1400 Loss 0.0885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Save models weights\n",
            "INFO:__main__:Save tokenizers\n",
            "INFO:__main__:Save model's metadata\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLMy8wdqxKy6",
        "colab_type": "text"
      },
      "source": [
        "## Evalute on different instances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvoX_jGqvu5_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "46d43246-e332-4425-c4ff-d3ed50d2b7c4"
      },
      "source": [
        "input_tokenizer = load_keras_tokenizer_json(model_path / \"source_tokenizer.json\")\n",
        "output_tokenizer = load_keras_tokenizer_json(model_path / \"target_tokenizer.json\")\n",
        "model_metadata = load_model_metadata(model_path / \"model_metadata.json\")\n",
        "encoder, _, decoder = get_model(\n",
        "    model_metadata[\"vocab_inp_size\"],\n",
        "    model_metadata[\"vocab_tar_size\"],\n",
        "    model_metadata[\"embedding_dim\"],\n",
        "    model_metadata[\"units\"],\n",
        "    model_metadata[\"batch_size\"],\n",
        ")\n",
        "encoder.load_weights((model_path / \"encoder/checkpoint\").as_posix())\n",
        "decoder.load_weights((model_path / \"decoder/checkpoint\").as_posix())\n",
        "\n",
        "names = {\n",
        "    \"Mohammed\": \"محمد‎\",\n",
        "    \"Mamun\": \"مامون\",\n",
        "    \"Urdu\": \"فیضان‎\",\n",
        "    \"Thomas\": \"توماس\",\n",
        "    \"Léna\": \"لينا\",\n",
        "    \"Jean\": \"جينز\",\n",
        "    \"Boubacar\": \"بوبكر\",\n",
        "    \"Ghita\": \"غيتا\",\n",
        "    \"Ezékiel\": \"حزقيال\",\n",
        "    \"Gaspard\": \"جاسبارد\",\n",
        "    \"Balthasar\": \"بالتازار\",\n",
        "    \"Olivier\": \"أوليفر\",\n",
        "    \"Jason\": \"جيسون\",\n",
        "    \"Nicolas\": \"نيكولاس\",\n",
        "    \"George\": \"جورج\",\n",
        "    \"Joséphine\": \"جوزفين\",\n",
        "    \"Cunégonde\": \"كونيجوند\",\n",
        "    \"Hortense\": \"هورتنس\",\n",
        "    \"Boutros Boutros-Ghali\": \"بطرس بطرس غالي\",\n",
        "    \"Rifa'a al-Tahtawi\": \"رفاعة رافع الطهطاوي\",\n",
        "    \"Saad Zaghloul\": \"سعد زغلول‎\",\n",
        "    \"Farouk El-Baz\": \"فاروق الباز‎\",\n",
        "    \"Abū ʿAbdallāh Yaʿīsh ibn Ibrāhīm ibn Yūsuf ibn Simāk al-Andalusī al-Umawī\": \"يعيش بن إبراهيم بن يوسف بن سماك الأموي الأندلسي\",\n",
        "    \"Ahmed Hassan Zewail\": \"أحمد حسن زويل‎\",\n",
        "    \"Abdel-Wahed El-Wakil\": \"عبد الواحد الوكيل‎\",\n",
        "    \"Suad Amiry\": \"سعاد العامري‎\",\n",
        "    \"Aḥmad ibn Faḍlān ibn al-ʿAbbās ibn Rāšid ibn Ḥammād\": \"أحمد بن فضلان بن العباس بن راشد بن حماد‎\",\n",
        "    \"Ahmad ibn Mājid\": \"أحمد بن ماجد\",\n",
        "    \"Abbas Mahmoud al-Aqqad\": \"عباس محمود العقاد‎\",\n",
        "    \"Imru' al-Qais Junduh bin Hujr al-Kindi \": \"ٱمْرُؤ ٱلْقَيْس جُنْدُح ٱبْن حُجْر ٱلْكِنْدِيّ‎\",\n",
        "    \"Abū al-Qāsim Khalaf ibn al-'Abbās al-Zahrāwī al-Ansari\": \"أبو القاسم خلف بن العباس الزهراوي\",\n",
        "}\n",
        "\n",
        "for latin, arabic in names.items():\n",
        "    transliterated = transliterate(\n",
        "        name=arabic,\n",
        "        input_tokenizer=input_tokenizer,\n",
        "        output_tokenizer=output_tokenizer,\n",
        "        encoder=encoder,\n",
        "        decoder=decoder,\n",
        "        metadata=model_metadata,\n",
        "    )\n",
        "    print(f\"Original      : {arabic}\")\n",
        "    print(f\"Transliterated: {transliterated}\")\n",
        "    print(f\"Ground truth  : {latin}\")\n",
        "    print(\"-------\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original      : محمد‎\n",
            "Transliterated: mahamed\n",
            "Ground truth  : Mohammed\n",
            "-------\n",
            "Original      : مامون\n",
            "Transliterated: mamoon\n",
            "Ground truth  : Mamun\n",
            "-------\n",
            "Original      : فیضان‎\n",
            "Transliterated: vidan\n",
            "Ground truth  : Urdu\n",
            "-------\n",
            "Original      : توماس\n",
            "Transliterated: tuomas\n",
            "Ground truth  : Thomas\n",
            "-------\n",
            "Original      : لينا\n",
            "Transliterated: lina\n",
            "Ground truth  : Léna\n",
            "-------\n",
            "Original      : جينز\n",
            "Transliterated: jenes\n",
            "Ground truth  : Jean\n",
            "-------\n",
            "Original      : بوبكر\n",
            "Transliterated: boubacar\n",
            "Ground truth  : Boubacar\n",
            "-------\n",
            "Original      : غيتا\n",
            "Transliterated: gheta\n",
            "Ground truth  : Ghita\n",
            "-------\n",
            "Original      : حزقيال\n",
            "Transliterated: hazaghall\n",
            "Ground truth  : Ezékiel\n",
            "-------\n",
            "Original      : جاسبارد\n",
            "Transliterated: gasbard\n",
            "Ground truth  : Gaspard\n",
            "-------\n",
            "Original      : بالتازار\n",
            "Transliterated: baltazar\n",
            "Ground truth  : Balthasar\n",
            "-------\n",
            "Original      : أوليفر\n",
            "Transliterated: olliver\n",
            "Ground truth  : Olivier\n",
            "-------\n",
            "Original      : جيسون\n",
            "Transliterated: jeison\n",
            "Ground truth  : Jason\n",
            "-------\n",
            "Original      : نيكولاس\n",
            "Transliterated: nikolaus\n",
            "Ground truth  : Nicolas\n",
            "-------\n",
            "Original      : جورج\n",
            "Transliterated: djurger\n",
            "Ground truth  : George\n",
            "-------\n",
            "Original      : جوزفين\n",
            "Transliterated: josephin\n",
            "Ground truth  : Joséphine\n",
            "-------\n",
            "Original      : كونيجوند\n",
            "Transliterated: kunigunde\n",
            "Ground truth  : Cunégonde\n",
            "-------\n",
            "Original      : هورتنس\n",
            "Transliterated: hortons\n",
            "Ground truth  : Hortense\n",
            "-------\n",
            "Original      : بطرس بطرس غالي\n",
            "Transliterated: butrus butrus galley\n",
            "Ground truth  : Boutros Boutros-Ghali\n",
            "-------\n",
            "Original      : رفاعة رافع الطهطاوي\n",
            "Transliterated: rafae rafe tatawy\n",
            "Ground truth  : Rifa'a al-Tahtawi\n",
            "-------\n",
            "Original      : سعد زغلول‎\n",
            "Transliterated: sa'ad zaghlool\n",
            "Ground truth  : Saad Zaghloul\n",
            "-------\n",
            "Original      : فاروق الباز‎\n",
            "Transliterated: farooq albaz\n",
            "Ground truth  : Farouk El-Baz\n",
            "-------\n",
            "Original      : يعيش بن إبراهيم بن يوسف بن سماك الأموي الأندلسي\n",
            "Transliterated: yaich ben abrahim ben yusef ben smack alamoy landolsi\n",
            "Ground truth  : Abū ʿAbdallāh Yaʿīsh ibn Ibrāhīm ibn Yūsuf ibn Simāk al-Andalusī al-Umawī\n",
            "-------\n",
            "Original      : أحمد حسن زويل‎\n",
            "Transliterated: akhmed hassan zewail\n",
            "Ground truth  : Ahmed Hassan Zewail\n",
            "-------\n",
            "Original      : عبد الواحد الوكيل‎\n",
            "Transliterated: abdulredalam waahid loken\n",
            "Ground truth  : Abdel-Wahed El-Wakil\n",
            "-------\n",
            "Original      : سعاد العامري‎\n",
            "Transliterated: suaad amri\n",
            "Ground truth  : Suad Amiry\n",
            "-------\n",
            "Original      : أحمد بن فضلان بن العباس بن راشد بن حماد‎\n",
            "Transliterated: akhmed ben fahdlan ben abbas ben rashed ben hamad\n",
            "Ground truth  : Aḥmad ibn Faḍlān ibn al-ʿAbbās ibn Rāšid ibn Ḥammād\n",
            "-------\n",
            "Original      : أحمد بن ماجد\n",
            "Transliterated: akhmed ben maged\n",
            "Ground truth  : Ahmad ibn Mājid\n",
            "-------\n",
            "Original      : عباس محمود العقاد‎\n",
            "Transliterated: abbas mahmood akkad\n",
            "Ground truth  : Abbas Mahmoud al-Aqqad\n",
            "-------\n",
            "Original      : ٱمْرُؤ ٱلْقَيْس جُنْدُح ٱبْن حُجْر ٱلْكِنْدِيّ‎\n",
            "Transliterated: emere alkais jinda ibn hajar kindi\n",
            "Ground truth  : Imru' al-Qais Junduh bin Hujr al-Kindi \n",
            "-------\n",
            "Original      : أبو القاسم خلف بن العباس الزهراوي\n",
            "Transliterated: abo qasim khalf ben abbas zahrawi\n",
            "Ground truth  : Abū al-Qāsim Khalaf ibn al-'Abbās al-Zahrāwī al-Ansari\n",
            "-------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
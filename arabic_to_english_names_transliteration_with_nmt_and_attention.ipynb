{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of arabic_to_english_names_transliteration_with_nmt_and_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomas-chauvet/names_transliteration/blob/master/arabic_to_english_names_transliteration_with_nmt_and_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Names transliteration - Neural machine translation with attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiwtNgENbx2g"
      },
      "source": [
        "This notebook trains a sequence to sequence (seq2seq) model to translitate names with arabic characters to names in latin character. Usually we call this task \"romanization\". It is the task to transform string from one alphabet to latin alphanet.\n",
        "\n",
        "After training the model in this notebook, you will be able to input an arabic name, such as *محمد‎*, and return the transliteration/translation of this name: *mohammad*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuWAqgLEw8zX"
      },
      "source": [
        "## Clone repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POgIZ7lWt-Jf",
        "outputId": "67f69aa2-bd1a-4247-a45d-b8da122d101c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "! rm -rf names_transliteration/\n",
        "! git clone https://github.com/thomas-chauvet/names_transliteration.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'names_transliteration'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Counting objects: 100% (83/83), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 83 (delta 22), reused 51 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (83/83), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ns2feAxw-59"
      },
      "source": [
        "## Install python library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RPyxLNSsiKN",
        "outputId": "acabdae0-c207-4566-cac2-c78deca82563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cd names_transliteration/ && python setup.py install"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating names_transliteration.egg-info\n",
            "writing names_transliteration.egg-info/PKG-INFO\n",
            "writing dependency_links to names_transliteration.egg-info/dependency_links.txt\n",
            "writing entry points to names_transliteration.egg-info/entry_points.txt\n",
            "writing requirements to names_transliteration.egg-info/requires.txt\n",
            "writing top-level names to names_transliteration.egg-info/top_level.txt\n",
            "writing manifest file 'names_transliteration.egg-info/SOURCES.txt'\n",
            "writing manifest file 'names_transliteration.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/transliteration\n",
            "copying transliteration/train_nmt.py -> build/lib/transliteration\n",
            "copying transliteration/get_data.py -> build/lib/transliteration\n",
            "copying transliteration/transliterate_name.py -> build/lib/transliteration\n",
            "copying transliteration/__init__.py -> build/lib/transliteration\n",
            "creating build/lib/transliteration/model\n",
            "copying transliteration/model/save.py -> build/lib/transliteration/model\n",
            "copying transliteration/model/train.py -> build/lib/transliteration/model\n",
            "copying transliteration/model/transliterate.py -> build/lib/transliteration/model\n",
            "copying transliteration/model/process.py -> build/lib/transliteration/model\n",
            "copying transliteration/model/__init__.py -> build/lib/transliteration/model\n",
            "creating build/lib/transliteration/collect\n",
            "copying transliteration/collect/process.py -> build/lib/transliteration/collect\n",
            "copying transliteration/collect/download.py -> build/lib/transliteration/collect\n",
            "copying transliteration/collect/__init__.py -> build/lib/transliteration/collect\n",
            "creating build/lib/transliteration/cleaning\n",
            "copying transliteration/cleaning/__init__.py -> build/lib/transliteration/cleaning\n",
            "copying transliteration/cleaning/arabic.py -> build/lib/transliteration/cleaning\n",
            "creating build/lib/transliteration/model/nmt\n",
            "copying transliteration/model/nmt/encoder.py -> build/lib/transliteration/model/nmt\n",
            "copying transliteration/model/nmt/attention.py -> build/lib/transliteration/model/nmt\n",
            "copying transliteration/model/nmt/decoder.py -> build/lib/transliteration/model/nmt\n",
            "copying transliteration/model/nmt/loss.py -> build/lib/transliteration/model/nmt\n",
            "copying transliteration/model/nmt/__init__.py -> build/lib/transliteration/model/nmt\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/transliteration\n",
            "creating build/bdist.linux-x86_64/egg/transliteration/model\n",
            "creating build/bdist.linux-x86_64/egg/transliteration/model/nmt\n",
            "copying build/lib/transliteration/model/nmt/encoder.py -> build/bdist.linux-x86_64/egg/transliteration/model/nmt\n",
            "copying build/lib/transliteration/model/nmt/attention.py -> build/bdist.linux-x86_64/egg/transliteration/model/nmt\n",
            "copying build/lib/transliteration/model/nmt/decoder.py -> build/bdist.linux-x86_64/egg/transliteration/model/nmt\n",
            "copying build/lib/transliteration/model/nmt/loss.py -> build/bdist.linux-x86_64/egg/transliteration/model/nmt\n",
            "copying build/lib/transliteration/model/nmt/__init__.py -> build/bdist.linux-x86_64/egg/transliteration/model/nmt\n",
            "copying build/lib/transliteration/model/save.py -> build/bdist.linux-x86_64/egg/transliteration/model\n",
            "copying build/lib/transliteration/model/train.py -> build/bdist.linux-x86_64/egg/transliteration/model\n",
            "copying build/lib/transliteration/model/transliterate.py -> build/bdist.linux-x86_64/egg/transliteration/model\n",
            "copying build/lib/transliteration/model/process.py -> build/bdist.linux-x86_64/egg/transliteration/model\n",
            "copying build/lib/transliteration/model/__init__.py -> build/bdist.linux-x86_64/egg/transliteration/model\n",
            "copying build/lib/transliteration/train_nmt.py -> build/bdist.linux-x86_64/egg/transliteration\n",
            "creating build/bdist.linux-x86_64/egg/transliteration/collect\n",
            "copying build/lib/transliteration/collect/process.py -> build/bdist.linux-x86_64/egg/transliteration/collect\n",
            "copying build/lib/transliteration/collect/download.py -> build/bdist.linux-x86_64/egg/transliteration/collect\n",
            "copying build/lib/transliteration/collect/__init__.py -> build/bdist.linux-x86_64/egg/transliteration/collect\n",
            "creating build/bdist.linux-x86_64/egg/transliteration/cleaning\n",
            "copying build/lib/transliteration/cleaning/__init__.py -> build/bdist.linux-x86_64/egg/transliteration/cleaning\n",
            "copying build/lib/transliteration/cleaning/arabic.py -> build/bdist.linux-x86_64/egg/transliteration/cleaning\n",
            "copying build/lib/transliteration/get_data.py -> build/bdist.linux-x86_64/egg/transliteration\n",
            "copying build/lib/transliteration/transliterate_name.py -> build/bdist.linux-x86_64/egg/transliteration\n",
            "copying build/lib/transliteration/__init__.py -> build/bdist.linux-x86_64/egg/transliteration\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/model/nmt/encoder.py to encoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/model/nmt/attention.py to attention.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/model/nmt/decoder.py to decoder.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/model/nmt/loss.py to loss.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/model/nmt/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/model/save.py to save.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/model/train.py to train.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/model/transliterate.py to transliterate.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/model/process.py to process.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/model/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/train_nmt.py to train_nmt.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/collect/process.py to process.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/collect/download.py to download.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/collect/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/cleaning/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/cleaning/arabic.py to arabic.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/get_data.py to get_data.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/transliterate_name.py to transliterate_name.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/transliteration/__init__.py to __init__.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying names_transliteration.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying names_transliteration.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying names_transliteration.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying names_transliteration.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying names_transliteration.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying names_transliteration.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "transliteration.__pycache__.__init__.cpython-36: module references __file__\n",
            "creating dist\n",
            "creating 'dist/names_transliteration-0.0.0-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing names_transliteration-0.0.0-py3.6.egg\n",
            "removing '/usr/local/lib/python3.6/dist-packages/names_transliteration-0.0.0-py3.6.egg' (and everything under it)\n",
            "creating /usr/local/lib/python3.6/dist-packages/names_transliteration-0.0.0-py3.6.egg\n",
            "Extracting names_transliteration-0.0.0-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "names-transliteration 0.0.0 is already the active version in easy-install.pth\n",
            "Installing names-transliteration-get-data script to /usr/local/bin\n",
            "Installing names-transliteration-train script to /usr/local/bin\n",
            "Installing names-transliteration-transliterate script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/names_transliteration-0.0.0-py3.6.egg\n",
            "Processing dependencies for names-transliteration==0.0.0\n",
            "Searching for keras-applications>=1.0.8\n",
            "Reading https://pypi.org/simple/keras-applications/\n",
            "Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl#sha256=df4323692b8c1174af821bf906f1e442e63fa7589bf0f1230a0b6bdc5a810c95\n",
            "Best match: Keras-Applications 1.0.8\n",
            "Processing Keras_Applications-1.0.8-py3-none-any.whl\n",
            "Installing Keras_Applications-1.0.8-py3-none-any.whl to /usr/local/lib/python3.6/dist-packages\n",
            "Adding Keras-Applications 1.0.8 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/Keras_Applications-1.0.8-py3.6.egg\n",
            "Searching for gast==0.2.2\n",
            "Reading https://pypi.org/simple/gast/\n",
            "Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz#sha256=fe939df4583692f0512161ec1c880e0a10e71e6a232da045ab8edd3756fbadf0\n",
            "Best match: gast 0.2.2\n",
            "Processing gast-0.2.2.tar.gz\n",
            "Writing /tmp/easy_install-yzc3be90/gast-0.2.2/setup.cfg\n",
            "Running gast-0.2.2/setup.py -q bdist_egg --dist-dir /tmp/easy_install-yzc3be90/gast-0.2.2/egg-dist-tmp-98cww5iu\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "Moving gast-0.2.2-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding gast 0.2.2 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/gast-0.2.2-py3.6.egg\n",
            "Searching for pandas==1.1.0\n",
            "Best match: pandas 1.1.0\n",
            "Processing pandas-1.1.0-py3.6-linux-x86_64.egg\n",
            "pandas 1.1.0 is already the active version in easy-install.pth\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages/pandas-1.1.0-py3.6-linux-x86_64.egg\n",
            "Searching for tensorflow==2.1.0\n",
            "Best match: tensorflow 2.1.0\n",
            "Processing tensorflow-2.1.0-py3.6-linux-x86_64.egg\n",
            "tensorflow 2.1.0 is already the active version in easy-install.pth\n",
            "Installing estimator_ckpt_converter script to /usr/local/bin\n",
            "Installing saved_model_cli script to /usr/local/bin\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "Installing tf_upgrade_v2 script to /usr/local/bin\n",
            "Installing tflite_convert script to /usr/local/bin\n",
            "Installing toco script to /usr/local/bin\n",
            "Installing toco_from_protos script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages/tensorflow-2.1.0-py3.6-linux-x86_64.egg\n",
            "Searching for pytz==2018.9\n",
            "Best match: pytz 2018.9\n",
            "Adding pytz 2018.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for python-dateutil==2.8.1\n",
            "Best match: python-dateutil 2.8.1\n",
            "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for numpy==1.18.5\n",
            "Best match: numpy 1.18.5\n",
            "Adding numpy 1.18.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.6 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for wrapt==1.12.1\n",
            "Best match: wrapt 1.12.1\n",
            "Adding wrapt 1.12.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for wheel==0.35.1\n",
            "Best match: wheel 0.35.1\n",
            "Adding wheel 0.35.1 to easy-install.pth file\n",
            "Installing wheel script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for termcolor==1.1.0\n",
            "Best match: termcolor 1.1.0\n",
            "Adding termcolor 1.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for tensorflow-estimator==2.1.0\n",
            "Best match: tensorflow-estimator 2.1.0\n",
            "Processing tensorflow_estimator-2.1.0-py3.6.egg\n",
            "tensorflow-estimator 2.1.0 is already the active version in easy-install.pth\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages/tensorflow_estimator-2.1.0-py3.6.egg\n",
            "Searching for tensorboard==2.1.1\n",
            "Best match: tensorboard 2.1.1\n",
            "Processing tensorboard-2.1.1-py3.6.egg\n",
            "tensorboard 2.1.1 is already the active version in easy-install.pth\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages/tensorboard-2.1.1-py3.6.egg\n",
            "Searching for six==1.15.0\n",
            "Best match: six 1.15.0\n",
            "Adding six 1.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for scipy==1.4.1\n",
            "Best match: scipy 1.4.1\n",
            "Adding scipy 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for protobuf==3.12.4\n",
            "Best match: protobuf 3.12.4\n",
            "Adding protobuf 3.12.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for opt-einsum==3.3.0\n",
            "Best match: opt-einsum 3.3.0\n",
            "Adding opt-einsum 3.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Keras-Preprocessing==1.1.2\n",
            "Best match: Keras-Preprocessing 1.1.2\n",
            "Adding Keras-Preprocessing 1.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for grpcio==1.32.0\n",
            "Best match: grpcio 1.32.0\n",
            "Adding grpcio 1.32.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for google-pasta==0.2.0\n",
            "Best match: google-pasta 0.2.0\n",
            "Adding google-pasta 0.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for astor==0.8.1\n",
            "Best match: astor 0.8.1\n",
            "Adding astor 0.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for absl-py==0.10.0\n",
            "Best match: absl-py 0.10.0\n",
            "Adding absl-py 0.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Werkzeug==1.0.1\n",
            "Best match: Werkzeug 1.0.1\n",
            "Adding Werkzeug 1.0.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for setuptools==50.3.0\n",
            "Best match: setuptools 50.3.0\n",
            "Adding setuptools 50.3.0 to easy-install.pth file\n",
            "Installing easy_install script to /usr/local/bin\n",
            "Installing easy_install-3.8 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for requests==2.23.0\n",
            "Best match: requests 2.23.0\n",
            "Adding requests 2.23.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for Markdown==3.2.2\n",
            "Best match: Markdown 3.2.2\n",
            "Adding Markdown 3.2.2 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for google-auth==1.17.2\n",
            "Best match: google-auth 1.17.2\n",
            "Adding google-auth 1.17.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for google-auth-oauthlib==0.4.1\n",
            "Best match: google-auth-oauthlib 0.4.1\n",
            "Adding google-auth-oauthlib 0.4.1 to easy-install.pth file\n",
            "Installing google-oauthlib-tool script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for h5py==2.10.0\n",
            "Best match: h5py 2.10.0\n",
            "Adding h5py 2.10.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for chardet==3.0.4\n",
            "Best match: chardet 3.0.4\n",
            "Adding chardet 3.0.4 to easy-install.pth file\n",
            "Installing chardetect script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for idna==2.10\n",
            "Best match: idna 2.10\n",
            "Adding idna 2.10 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for certifi==2020.6.20\n",
            "Best match: certifi 2020.6.20\n",
            "Adding certifi 2020.6.20 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for urllib3==1.24.3\n",
            "Best match: urllib3 1.24.3\n",
            "Adding urllib3 1.24.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for importlib-metadata==2.0.0\n",
            "Best match: importlib-metadata 2.0.0\n",
            "Adding importlib-metadata 2.0.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for cachetools==4.1.1\n",
            "Best match: cachetools 4.1.1\n",
            "Adding cachetools 4.1.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for rsa==4.6\n",
            "Best match: rsa 4.6\n",
            "Adding rsa 4.6 to easy-install.pth file\n",
            "Installing pyrsa-decrypt script to /usr/local/bin\n",
            "Installing pyrsa-encrypt script to /usr/local/bin\n",
            "Installing pyrsa-keygen script to /usr/local/bin\n",
            "Installing pyrsa-priv2pub script to /usr/local/bin\n",
            "Installing pyrsa-sign script to /usr/local/bin\n",
            "Installing pyrsa-verify script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyasn1-modules==0.2.8\n",
            "Best match: pyasn1-modules 0.2.8\n",
            "Adding pyasn1-modules 0.2.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for requests-oauthlib==1.3.0\n",
            "Best match: requests-oauthlib 1.3.0\n",
            "Adding requests-oauthlib 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for zipp==3.2.0\n",
            "Best match: zipp 3.2.0\n",
            "Adding zipp 3.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for pyasn1==0.4.8\n",
            "Best match: pyasn1 0.4.8\n",
            "Adding pyasn1 0.4.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Searching for oauthlib==3.1.0\n",
            "Best match: oauthlib 3.1.0\n",
            "Adding oauthlib 3.1.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.6/dist-packages\n",
            "Finished processing dependencies for names-transliteration==0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXQ8KgsghgRz"
      },
      "source": [
        "# Link to google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6uYclUahfly",
        "outputId": "e553d216-1cef-4615-b5d3-f78ad097bff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8EkZ1oCxBEs"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImaLXZO1thn8"
      },
      "source": [
        "from transliteration.model.nmt import get_model\n",
        "from transliteration.model.process import load_dataset\n",
        "from transliteration.model.save import save_keras_tokenizer_json, save_model_metadata\n",
        "from transliteration.model.train import train\n",
        "from transliteration.model.save import load_keras_tokenizer_json, load_model_metadata\n",
        "from transliteration.model.transliterate import transliterate\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "logging.basicConfig()\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kER_sgpxDOA"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tEKC7vhuk54"
      },
      "source": [
        "num_examples = None\n",
        "test_size = 0.2\n",
        "batch_size = 64\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "epochs = 20\n",
        "\n",
        "model_path = Path(\"/content/drive//My Drive/\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0qRetNbxIqW"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxXKDjq3jEL",
        "outputId": "7c98ce75-ec2a-45c9-9892-3e88e616e48b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "logger.info(\"Load dataset\")\n",
        "source_tensor, target_tensor, source, target = load_dataset(\n",
        "    Path(\"names_transliteration/data/clean/arabic_english.csv\"), num_examples=num_examples\n",
        ")\n",
        "\n",
        "logger.info(\"Save tokenizers\")\n",
        "save_keras_tokenizer_json(source, model_path / \"source_tokenizer.json\")\n",
        "save_keras_tokenizer_json(target, model_path / \"target_tokenizer.json\")\n",
        "\n",
        "logger.info(\"Creating training and validation sets\")\n",
        "(\n",
        "    source_tensor_train,\n",
        "    source_tensor_val,\n",
        "    target_tensor_train,\n",
        "    target_tensor_val,\n",
        ") = train_test_split(source_tensor, target_tensor, test_size=test_size)\n",
        "\n",
        "BUFFER_SIZE = len(source_tensor_train)\n",
        "logger.info(f\"BUFFER_SIZE: {BUFFER_SIZE}\")\n",
        "STEPS_PER_EPOCH = len(source_tensor_train) // batch_size\n",
        "logger.info(f\"STEPS_PER_EPOCH: {STEPS_PER_EPOCH}\")\n",
        "VOCAB_INP_SIZE = len(source.word_index) + 1\n",
        "logger.info(f\"VOCAB_INP_SIZE: {VOCAB_INP_SIZE}\")\n",
        "VOCAB_TAR_SIZE = len(target.word_index) + 1\n",
        "logger.info(f\"VOCAB_TAR_SIZE: {VOCAB_TAR_SIZE}\")\n",
        "\n",
        "metadata = {\n",
        "    \"batch_size\": batch_size,\n",
        "    \"embedding_dim\": embedding_dim,\n",
        "    \"units\": units,\n",
        "    \"vocab_inp_size\": len(source.word_index) + 1,\n",
        "    \"vocab_tar_size\": len(target.word_index) + 1,\n",
        "    \"max_length_source\": source_tensor.shape[1],\n",
        "    \"max_length_target\": target_tensor.shape[1],\n",
        "}\n",
        "\n",
        "logger.info(\"Save model's metadata\")\n",
        "save_model_metadata(metadata, model_path / \"model_metadata.json\")\n",
        "\n",
        "logger.info(\"Tensorflow dataset batch\")\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (source_tensor_train, target_tensor_train)\n",
        ").shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "logger.info(\"Instanciate encoder, attention and decoder\")\n",
        "encoder, attention_layer, decoder = get_model(\n",
        "    VOCAB_INP_SIZE, VOCAB_TAR_SIZE, embedding_dim=embedding_dim, units=units, batch_sz=batch_size\n",
        ")\n",
        "\n",
        "logger.info(\"Train model\")\n",
        "encoder, decoder = train(\n",
        "    dataset,\n",
        "    encoder,\n",
        "    decoder,\n",
        "    target,\n",
        "    STEPS_PER_EPOCH,\n",
        "    epochs=epochs,\n",
        "    checkpoint_dir=model_path / \"training_checkpoints\",\n",
        ")\n",
        "\n",
        "logger.info(\"Save models weights\")\n",
        "encoder.save_weights(\n",
        "    (model_path / \"encoder/checkpoint\").as_posix(), save_format=\"tf\"\n",
        ")\n",
        "decoder.save_weights(\n",
        "    (model_path / \"decoder/checkpoint\").as_posix(), save_format=\"tf\"\n",
        ")\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Load dataset\n",
            "INFO:__main__:Save tokenizers\n",
            "INFO:__main__:Creating training and validation sets\n",
            "INFO:__main__:BUFFER_SIZE: 94438\n",
            "INFO:__main__:STEPS_PER_EPOCH: 1475\n",
            "INFO:__main__:VOCAB_INP_SIZE: 55\n",
            "INFO:__main__:VOCAB_TAR_SIZE: 60\n",
            "INFO:__main__:Save model's metadata\n",
            "INFO:__main__:Tensorflow dataset batch\n",
            "INFO:__main__:Instanciate encoder, attention and decoder\n",
            "INFO:__main__:Train model\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.2066\n",
            "Epoch 1 Batch 100 Loss 0.7683\n",
            "Epoch 1 Batch 200 Loss 0.5617\n",
            "Epoch 1 Batch 300 Loss 0.2970\n",
            "Epoch 1 Batch 400 Loss 0.2902\n",
            "Epoch 1 Batch 500 Loss 0.2224\n",
            "Epoch 1 Batch 600 Loss 0.3280\n",
            "Epoch 1 Batch 700 Loss 0.2446\n",
            "Epoch 1 Batch 800 Loss 0.2455\n",
            "Epoch 1 Batch 900 Loss 0.2580\n",
            "Epoch 1 Batch 1000 Loss 0.2527\n",
            "Epoch 1 Batch 1100 Loss 0.2387\n",
            "Epoch 1 Batch 1200 Loss 0.2234\n",
            "Epoch 1 Batch 1300 Loss 0.2964\n",
            "Epoch 1 Batch 1400 Loss 0.2075\n",
            "Epoch 2 Batch 0 Loss 0.2522\n",
            "Epoch 2 Batch 100 Loss 0.2376\n",
            "Epoch 2 Batch 200 Loss 0.1977\n",
            "Epoch 2 Batch 300 Loss 0.2659\n",
            "Epoch 2 Batch 400 Loss 0.2061\n",
            "Epoch 2 Batch 500 Loss 0.2162\n",
            "Epoch 2 Batch 600 Loss 0.1943\n",
            "Epoch 2 Batch 700 Loss 0.1964\n",
            "Epoch 2 Batch 800 Loss 0.2032\n",
            "Epoch 2 Batch 900 Loss 0.2136\n",
            "Epoch 2 Batch 1000 Loss 0.2091\n",
            "Epoch 2 Batch 1100 Loss 0.1823\n",
            "Epoch 2 Batch 1200 Loss 0.1864\n",
            "Epoch 2 Batch 1300 Loss 0.1878\n",
            "Epoch 2 Batch 1400 Loss 0.1720\n",
            "Epoch 3 Batch 0 Loss 0.1933\n",
            "Epoch 3 Batch 100 Loss 0.2152\n",
            "Epoch 3 Batch 200 Loss 0.2103\n",
            "Epoch 3 Batch 300 Loss 0.1690\n",
            "Epoch 3 Batch 400 Loss 0.1881\n",
            "Epoch 3 Batch 500 Loss 0.2019\n",
            "Epoch 3 Batch 600 Loss 0.1791\n",
            "Epoch 3 Batch 700 Loss 0.1699\n",
            "Epoch 3 Batch 800 Loss 0.2113\n",
            "Epoch 3 Batch 900 Loss 0.1925\n",
            "Epoch 3 Batch 1000 Loss 0.2178\n",
            "Epoch 3 Batch 1100 Loss 0.1761\n",
            "Epoch 3 Batch 1200 Loss 0.1852\n",
            "Epoch 3 Batch 1300 Loss 0.1508\n",
            "Epoch 3 Batch 1400 Loss 0.2102\n",
            "Epoch 4 Batch 0 Loss 0.1651\n",
            "Epoch 4 Batch 100 Loss 0.2290\n",
            "Epoch 4 Batch 200 Loss 0.1746\n",
            "Epoch 4 Batch 300 Loss 0.1941\n",
            "Epoch 4 Batch 400 Loss 0.1750\n",
            "Epoch 4 Batch 500 Loss 0.1709\n",
            "Epoch 4 Batch 600 Loss 0.1508\n",
            "Epoch 4 Batch 700 Loss 0.1710\n",
            "Epoch 4 Batch 800 Loss 0.1830\n",
            "Epoch 4 Batch 900 Loss 0.1907\n",
            "Epoch 4 Batch 1000 Loss 0.2146\n",
            "Epoch 4 Batch 1100 Loss 0.1733\n",
            "Epoch 4 Batch 1200 Loss 0.1956\n",
            "Epoch 4 Batch 1300 Loss 0.1773\n",
            "Epoch 4 Batch 1400 Loss 0.1851\n",
            "Epoch 5 Batch 0 Loss 0.1710\n",
            "Epoch 5 Batch 100 Loss 0.1596\n",
            "Epoch 5 Batch 200 Loss 0.1886\n",
            "Epoch 5 Batch 300 Loss 0.1495\n",
            "Epoch 5 Batch 400 Loss 0.1853\n",
            "Epoch 5 Batch 500 Loss 0.1660\n",
            "Epoch 5 Batch 600 Loss 0.1924\n",
            "Epoch 5 Batch 700 Loss 0.1857\n",
            "Epoch 5 Batch 800 Loss 0.1553\n",
            "Epoch 5 Batch 900 Loss 0.1701\n",
            "Epoch 5 Batch 1000 Loss 0.1687\n",
            "Epoch 5 Batch 1100 Loss 0.2086\n",
            "Epoch 5 Batch 1200 Loss 0.1676\n",
            "Epoch 5 Batch 1300 Loss 0.1532\n",
            "Epoch 5 Batch 1400 Loss 0.1872\n",
            "Epoch 6 Batch 0 Loss 0.1594\n",
            "Epoch 6 Batch 100 Loss 0.1313\n",
            "Epoch 6 Batch 200 Loss 0.1638\n",
            "Epoch 6 Batch 300 Loss 0.1753\n",
            "Epoch 6 Batch 400 Loss 0.1764\n",
            "Epoch 6 Batch 500 Loss 0.1529\n",
            "Epoch 6 Batch 600 Loss 0.1580\n",
            "Epoch 6 Batch 700 Loss 0.1712\n",
            "Epoch 6 Batch 800 Loss 0.1525\n",
            "Epoch 6 Batch 900 Loss 0.1934\n",
            "Epoch 6 Batch 1000 Loss 0.1607\n",
            "Epoch 6 Batch 1100 Loss 0.1394\n",
            "Epoch 6 Batch 1200 Loss 0.1929\n",
            "Epoch 6 Batch 1300 Loss 0.1564\n",
            "Epoch 6 Batch 1400 Loss 0.1824\n",
            "Epoch 7 Batch 0 Loss 0.1315\n",
            "Epoch 7 Batch 100 Loss 0.1798\n",
            "Epoch 7 Batch 200 Loss 0.1303\n",
            "Epoch 7 Batch 300 Loss 0.1592\n",
            "Epoch 7 Batch 400 Loss 0.1827\n",
            "Epoch 7 Batch 500 Loss 0.1389\n",
            "Epoch 7 Batch 600 Loss 0.1473\n",
            "Epoch 7 Batch 700 Loss 0.1925\n",
            "Epoch 7 Batch 800 Loss 0.1837\n",
            "Epoch 7 Batch 900 Loss 0.1470\n",
            "Epoch 7 Batch 1000 Loss 0.1401\n",
            "Epoch 7 Batch 1100 Loss 0.1407\n",
            "Epoch 7 Batch 1200 Loss 0.1595\n",
            "Epoch 7 Batch 1300 Loss 0.1769\n",
            "Epoch 7 Batch 1400 Loss 0.1606\n",
            "Epoch 8 Batch 0 Loss 0.1504\n",
            "Epoch 8 Batch 100 Loss 0.1378\n",
            "Epoch 8 Batch 200 Loss 0.1633\n",
            "Epoch 8 Batch 300 Loss 0.1237\n",
            "Epoch 8 Batch 400 Loss 0.1533\n",
            "Epoch 8 Batch 500 Loss 0.1490\n",
            "Epoch 8 Batch 600 Loss 0.1599\n",
            "Epoch 8 Batch 700 Loss 0.1188\n",
            "Epoch 8 Batch 800 Loss 0.1268\n",
            "Epoch 8 Batch 900 Loss 0.1544\n",
            "Epoch 8 Batch 1000 Loss 0.1455\n",
            "Epoch 8 Batch 1100 Loss 0.1653\n",
            "Epoch 8 Batch 1200 Loss 0.1574\n",
            "Epoch 8 Batch 1300 Loss 0.1206\n",
            "Epoch 8 Batch 1400 Loss 0.1770\n",
            "Epoch 9 Batch 0 Loss 0.1169\n",
            "Epoch 9 Batch 100 Loss 0.1167\n",
            "Epoch 9 Batch 200 Loss 0.1191\n",
            "Epoch 9 Batch 300 Loss 0.1193\n",
            "Epoch 9 Batch 400 Loss 0.1898\n",
            "Epoch 9 Batch 500 Loss 0.1302\n",
            "Epoch 9 Batch 600 Loss 0.1365\n",
            "Epoch 9 Batch 700 Loss 0.1226\n",
            "Epoch 9 Batch 800 Loss 0.1357\n",
            "Epoch 9 Batch 900 Loss 0.1744\n",
            "Epoch 9 Batch 1000 Loss 0.1430\n",
            "Epoch 9 Batch 1100 Loss 0.1535\n",
            "Epoch 9 Batch 1200 Loss 0.1809\n",
            "Epoch 9 Batch 1300 Loss 0.1157\n",
            "Epoch 9 Batch 1400 Loss 0.1436\n",
            "Epoch 10 Batch 0 Loss 0.1183\n",
            "Epoch 10 Batch 100 Loss 0.1054\n",
            "Epoch 10 Batch 200 Loss 0.1167\n",
            "Epoch 10 Batch 300 Loss 0.1033\n",
            "Epoch 10 Batch 400 Loss 0.0945\n",
            "Epoch 10 Batch 500 Loss 0.1137\n",
            "Epoch 10 Batch 600 Loss 0.1246\n",
            "Epoch 10 Batch 700 Loss 0.1138\n",
            "Epoch 10 Batch 800 Loss 0.1195\n",
            "Epoch 10 Batch 900 Loss 0.1314\n",
            "Epoch 10 Batch 1000 Loss 0.1244\n",
            "Epoch 10 Batch 1100 Loss 0.1323\n",
            "Epoch 10 Batch 1200 Loss 0.1068\n",
            "Epoch 10 Batch 1300 Loss 0.1399\n",
            "Epoch 10 Batch 1400 Loss 0.1465\n",
            "Epoch 11 Batch 0 Loss 0.1214\n",
            "Epoch 11 Batch 100 Loss 0.0975\n",
            "Epoch 11 Batch 200 Loss 0.0968\n",
            "Epoch 11 Batch 300 Loss 0.1189\n",
            "Epoch 11 Batch 400 Loss 0.1082\n",
            "Epoch 11 Batch 500 Loss 0.1166\n",
            "Epoch 11 Batch 600 Loss 0.1170\n",
            "Epoch 11 Batch 700 Loss 0.1191\n",
            "Epoch 11 Batch 800 Loss 0.1105\n",
            "Epoch 11 Batch 900 Loss 0.1190\n",
            "Epoch 11 Batch 1000 Loss 0.1124\n",
            "Epoch 11 Batch 1100 Loss 0.1531\n",
            "Epoch 11 Batch 1200 Loss 0.1261\n",
            "Epoch 11 Batch 1300 Loss 0.1106\n",
            "Epoch 11 Batch 1400 Loss 0.1225\n",
            "Epoch 12 Batch 0 Loss 0.0864\n",
            "Epoch 12 Batch 100 Loss 0.0941\n",
            "Epoch 12 Batch 200 Loss 0.0935\n",
            "Epoch 12 Batch 300 Loss 0.1042\n",
            "Epoch 12 Batch 400 Loss 0.1037\n",
            "Epoch 12 Batch 500 Loss 0.1037\n",
            "Epoch 12 Batch 600 Loss 0.0908\n",
            "Epoch 12 Batch 700 Loss 0.1092\n",
            "Epoch 12 Batch 800 Loss 0.0934\n",
            "Epoch 12 Batch 900 Loss 0.1175\n",
            "Epoch 12 Batch 1000 Loss 0.1052\n",
            "Epoch 12 Batch 1100 Loss 0.1091\n",
            "Epoch 12 Batch 1200 Loss 0.0963\n",
            "Epoch 12 Batch 1300 Loss 0.1017\n",
            "Epoch 12 Batch 1400 Loss 0.0962\n",
            "Epoch 13 Batch 0 Loss 0.1179\n",
            "Epoch 13 Batch 100 Loss 0.1184\n",
            "Epoch 13 Batch 200 Loss 0.1019\n",
            "Epoch 13 Batch 300 Loss 0.1153\n",
            "Epoch 13 Batch 400 Loss 0.1202\n",
            "Epoch 13 Batch 500 Loss 0.0995\n",
            "Epoch 13 Batch 600 Loss 0.0965\n",
            "Epoch 13 Batch 700 Loss 0.1068\n",
            "Epoch 13 Batch 800 Loss 0.1113\n",
            "Epoch 13 Batch 900 Loss 0.1173\n",
            "Epoch 13 Batch 1000 Loss 0.1038\n",
            "Epoch 13 Batch 1100 Loss 0.0912\n",
            "Epoch 13 Batch 1200 Loss 0.1004\n",
            "Epoch 13 Batch 1300 Loss 0.0877\n",
            "Epoch 13 Batch 1400 Loss 0.1079\n",
            "Epoch 14 Batch 0 Loss 0.0690\n",
            "Epoch 14 Batch 100 Loss 0.0832\n",
            "Epoch 14 Batch 200 Loss 0.0773\n",
            "Epoch 14 Batch 300 Loss 0.0887\n",
            "Epoch 14 Batch 400 Loss 0.0922\n",
            "Epoch 14 Batch 500 Loss 0.0678\n",
            "Epoch 14 Batch 600 Loss 0.0876\n",
            "Epoch 14 Batch 700 Loss 0.1265\n",
            "Epoch 14 Batch 800 Loss 0.1027\n",
            "Epoch 14 Batch 900 Loss 0.0852\n",
            "Epoch 14 Batch 1000 Loss 0.0936\n",
            "Epoch 14 Batch 1100 Loss 0.0830\n",
            "Epoch 14 Batch 1200 Loss 0.0976\n",
            "Epoch 14 Batch 1300 Loss 0.1072\n",
            "Epoch 14 Batch 1400 Loss 0.1098\n",
            "Epoch 15 Batch 0 Loss 0.0833\n",
            "Epoch 15 Batch 100 Loss 0.0865\n",
            "Epoch 15 Batch 200 Loss 0.0743\n",
            "Epoch 15 Batch 300 Loss 0.0949\n",
            "Epoch 15 Batch 400 Loss 0.1037\n",
            "Epoch 15 Batch 500 Loss 0.0962\n",
            "Epoch 15 Batch 600 Loss 0.0855\n",
            "Epoch 15 Batch 700 Loss 0.0917\n",
            "Epoch 15 Batch 800 Loss 0.0970\n",
            "Epoch 15 Batch 900 Loss 0.0794\n",
            "Epoch 15 Batch 1000 Loss 0.0836\n",
            "Epoch 15 Batch 1100 Loss 0.0974\n",
            "Epoch 15 Batch 1200 Loss 0.1020\n",
            "Epoch 15 Batch 1300 Loss 0.0987\n",
            "Epoch 15 Batch 1400 Loss 0.1043\n",
            "Epoch 16 Batch 0 Loss 0.0699\n",
            "Epoch 16 Batch 100 Loss 0.0766\n",
            "Epoch 16 Batch 200 Loss 0.0833\n",
            "Epoch 16 Batch 300 Loss 0.0847\n",
            "Epoch 16 Batch 400 Loss 0.0762\n",
            "Epoch 16 Batch 500 Loss 0.0895\n",
            "Epoch 16 Batch 600 Loss 0.0646\n",
            "Epoch 16 Batch 700 Loss 0.0850\n",
            "Epoch 16 Batch 800 Loss 0.1006\n",
            "Epoch 16 Batch 900 Loss 0.0924\n",
            "Epoch 16 Batch 1000 Loss 0.0815\n",
            "Epoch 16 Batch 1100 Loss 0.0959\n",
            "Epoch 16 Batch 1200 Loss 0.1038\n",
            "Epoch 16 Batch 1300 Loss 0.0816\n",
            "Epoch 16 Batch 1400 Loss 0.0887\n",
            "Epoch 17 Batch 0 Loss 0.0580\n",
            "Epoch 17 Batch 100 Loss 0.0661\n",
            "Epoch 17 Batch 200 Loss 0.0744\n",
            "Epoch 17 Batch 300 Loss 0.0707\n",
            "Epoch 17 Batch 400 Loss 0.0736\n",
            "Epoch 17 Batch 500 Loss 0.0580\n",
            "Epoch 17 Batch 600 Loss 0.0960\n",
            "Epoch 17 Batch 700 Loss 0.0631\n",
            "Epoch 17 Batch 800 Loss 0.0838\n",
            "Epoch 17 Batch 900 Loss 0.0800\n",
            "Epoch 17 Batch 1000 Loss 0.0985\n",
            "Epoch 17 Batch 1100 Loss 0.0832\n",
            "Epoch 17 Batch 1200 Loss 0.0954\n",
            "Epoch 17 Batch 1300 Loss 0.0819\n",
            "Epoch 17 Batch 1400 Loss 0.0642\n",
            "Epoch 18 Batch 0 Loss 0.0645\n",
            "Epoch 18 Batch 100 Loss 0.0793\n",
            "Epoch 18 Batch 200 Loss 0.0582\n",
            "Epoch 18 Batch 300 Loss 0.0806\n",
            "Epoch 18 Batch 400 Loss 0.0757\n",
            "Epoch 18 Batch 500 Loss 0.0824\n",
            "Epoch 18 Batch 600 Loss 0.0743\n",
            "Epoch 18 Batch 700 Loss 0.0645\n",
            "Epoch 18 Batch 800 Loss 0.0723\n",
            "Epoch 18 Batch 900 Loss 0.0893\n",
            "Epoch 18 Batch 1000 Loss 0.0718\n",
            "Epoch 18 Batch 1100 Loss 0.0792\n",
            "Epoch 18 Batch 1200 Loss 0.0903\n",
            "Epoch 18 Batch 1300 Loss 0.0943\n",
            "Epoch 18 Batch 1400 Loss 0.0839\n",
            "Epoch 19 Batch 0 Loss 0.0593\n",
            "Epoch 19 Batch 100 Loss 0.0567\n",
            "Epoch 19 Batch 200 Loss 0.0615\n",
            "Epoch 19 Batch 300 Loss 0.0689\n",
            "Epoch 19 Batch 400 Loss 0.0719\n",
            "Epoch 19 Batch 500 Loss 0.0808\n",
            "Epoch 19 Batch 600 Loss 0.0826\n",
            "Epoch 19 Batch 700 Loss 0.0733\n",
            "Epoch 19 Batch 800 Loss 0.0850\n",
            "Epoch 19 Batch 900 Loss 0.0764\n",
            "Epoch 19 Batch 1000 Loss 0.0770\n",
            "Epoch 19 Batch 1100 Loss 0.0651\n",
            "Epoch 19 Batch 1200 Loss 0.0820\n",
            "Epoch 19 Batch 1300 Loss 0.0807\n",
            "Epoch 19 Batch 1400 Loss 0.0891\n",
            "Epoch 20 Batch 0 Loss 0.0491\n",
            "Epoch 20 Batch 100 Loss 0.0599\n",
            "Epoch 20 Batch 200 Loss 0.0575\n",
            "Epoch 20 Batch 300 Loss 0.0572\n",
            "Epoch 20 Batch 400 Loss 0.0709\n",
            "Epoch 20 Batch 500 Loss 0.0782\n",
            "Epoch 20 Batch 600 Loss 0.0690\n",
            "Epoch 20 Batch 700 Loss 0.0780\n",
            "Epoch 20 Batch 800 Loss 0.0740\n",
            "Epoch 20 Batch 900 Loss 0.0629\n",
            "Epoch 20 Batch 1000 Loss 0.0949\n",
            "Epoch 20 Batch 1100 Loss 0.0787\n",
            "Epoch 20 Batch 1200 Loss 0.0862\n",
            "Epoch 20 Batch 1300 Loss 0.0597\n",
            "Epoch 20 Batch 1400 Loss 0.0994\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Save models weights\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLMy8wdqxKy6"
      },
      "source": [
        "## Evalute on different instances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvoX_jGqvu5_",
        "outputId": "0bcef809-8b4a-450e-d44e-24ffbe0f4b77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "input_tokenizer = load_keras_tokenizer_json(model_path / \"source_tokenizer.json\")\n",
        "output_tokenizer = load_keras_tokenizer_json(model_path / \"target_tokenizer.json\")\n",
        "model_metadata = load_model_metadata(model_path / \"model_metadata.json\")\n",
        "encoder, _, decoder = get_model(\n",
        "    model_metadata[\"vocab_inp_size\"],\n",
        "    model_metadata[\"vocab_tar_size\"],\n",
        "    model_metadata[\"embedding_dim\"],\n",
        "    model_metadata[\"units\"],\n",
        "    model_metadata[\"batch_size\"],\n",
        ")\n",
        "encoder.load_weights((model_path / \"encoder/checkpoint\").as_posix())\n",
        "decoder.load_weights((model_path / \"decoder/checkpoint\").as_posix())\n",
        "\n",
        "names = {\n",
        "    \"Mohammed\": \"محمد‎\",\n",
        "    \"Mamun\": \"مامون\",\n",
        "    \"Urdu\": \"فیضان‎\",\n",
        "    \"Thomas\": \"توماس\",\n",
        "    \"Léna\": \"لينا\",\n",
        "    \"Jean\": \"جينز\",\n",
        "    \"Boubacar\": \"بوبكر\",\n",
        "    \"Ghita\": \"غيتا\",\n",
        "    \"Ezékiel\": \"حزقيال\",\n",
        "    \"Gaspard\": \"جاسبارد\",\n",
        "    \"Balthasar\": \"بالتازار\",\n",
        "    \"Olivier\": \"أوليفر\",\n",
        "    \"Jason\": \"جيسون\",\n",
        "    \"Nicolas\": \"نيكولاس\",\n",
        "    \"George\": \"جورج\",\n",
        "    \"Joséphine\": \"جوزفين\",\n",
        "    \"Cunégonde\": \"كونيجوند\",\n",
        "    \"Hortense\": \"هورتنس\",\n",
        "    \"Boutros Boutros-Ghali\": \"بطرس بطرس غالي\",\n",
        "    \"Rifa'a al-Tahtawi\": \"رفاعة رافع الطهطاوي\",\n",
        "    \"Saad Zaghloul\": \"سعد زغلول‎\",\n",
        "    \"Farouk El-Baz\": \"فاروق الباز‎\",\n",
        "    \"Abū ʿAbdallāh Yaʿīsh ibn Ibrāhīm ibn Yūsuf ibn Simāk al-Andalusī al-Umawī\": \"يعيش بن إبراهيم بن يوسف بن سماك الأموي الأندلسي\",\n",
        "    \"Ahmed Hassan Zewail\": \"أحمد حسن زويل‎\",\n",
        "    \"Abdel-Wahed El-Wakil\": \"عبد الواحد الوكيل‎\",\n",
        "    \"Suad Amiry\": \"سعاد العامري‎\",\n",
        "    \"Aḥmad ibn Faḍlān ibn al-ʿAbbās ibn Rāšid ibn Ḥammād\": \"أحمد بن فضلان بن العباس بن راشد بن حماد‎\",\n",
        "    \"Ahmad ibn Mājid\": \"أحمد بن ماجد\",\n",
        "    \"Abbas Mahmoud al-Aqqad\": \"عباس محمود العقاد‎\",\n",
        "    \"Imru' al-Qais Junduh bin Hujr al-Kindi \": \"ٱمْرُؤ ٱلْقَيْس جُنْدُح ٱبْن حُجْر ٱلْكِنْدِيّ‎\",\n",
        "    \"Abū al-Qāsim Khalaf ibn al-'Abbās al-Zahrāwī al-Ansari\": \"أبو القاسم خلف بن العباس الزهراوي\",\n",
        "}\n",
        "\n",
        "for latin, arabic in names.items():\n",
        "    transliterated = transliterate(\n",
        "        name=arabic,\n",
        "        input_tokenizer=input_tokenizer,\n",
        "        output_tokenizer=output_tokenizer,\n",
        "        encoder=encoder,\n",
        "        decoder=decoder,\n",
        "        metadata=model_metadata,\n",
        "    )\n",
        "    print(f\"Original      : {arabic}\")\n",
        "    print(f\"Transliterated: {transliterated}\")\n",
        "    print(f\"Ground truth  : {latin}\")\n",
        "    print(\"-------\")\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original      : محمد‎\n",
            "Transliterated: mohammed\n",
            "Ground truth  : Mohammed\n",
            "-------\n",
            "Original      : مامون\n",
            "Transliterated: mammon\n",
            "Ground truth  : Mamun\n",
            "-------\n",
            "Original      : فیضان‎\n",
            "Transliterated: faidan\n",
            "Ground truth  : Urdu\n",
            "-------\n",
            "Original      : توماس\n",
            "Transliterated: thomas\n",
            "Ground truth  : Thomas\n",
            "-------\n",
            "Original      : لينا\n",
            "Transliterated: lena\n",
            "Ground truth  : Léna\n",
            "-------\n",
            "Original      : جينز\n",
            "Transliterated: ginz\n",
            "Ground truth  : Jean\n",
            "-------\n",
            "Original      : بوبكر\n",
            "Transliterated: boubakeur\n",
            "Ground truth  : Boubacar\n",
            "-------\n",
            "Original      : غيتا\n",
            "Transliterated: geta\n",
            "Ground truth  : Ghita\n",
            "-------\n",
            "Original      : حزقيال\n",
            "Transliterated: hazquial\n",
            "Ground truth  : Ezékiel\n",
            "-------\n",
            "Original      : جاسبارد\n",
            "Transliterated: gaspard\n",
            "Ground truth  : Gaspard\n",
            "-------\n",
            "Original      : بالتازار\n",
            "Transliterated: baltazar\n",
            "Ground truth  : Balthasar\n",
            "-------\n",
            "Original      : أوليفر\n",
            "Transliterated: oliver\n",
            "Ground truth  : Olivier\n",
            "-------\n",
            "Original      : جيسون\n",
            "Transliterated: jeison\n",
            "Ground truth  : Jason\n",
            "-------\n",
            "Original      : نيكولاس\n",
            "Transliterated: nicklaaoooooooooooooooooooo\n",
            "Ground truth  : Nicolas\n",
            "-------\n",
            "Original      : جورج\n",
            "Transliterated: george\n",
            "Ground truth  : George\n",
            "-------\n",
            "Original      : جوزفين\n",
            "Transliterated: josephin\n",
            "Ground truth  : Joséphine\n",
            "-------\n",
            "Original      : كونيجوند\n",
            "Transliterated: kunijond\n",
            "Ground truth  : Cunégonde\n",
            "-------\n",
            "Original      : هورتنس\n",
            "Transliterated: hortons\n",
            "Ground truth  : Hortense\n",
            "-------\n",
            "Original      : بطرس بطرس غالي\n",
            "Transliterated: botris botris ghaly\n",
            "Ground truth  : Boutros Boutros-Ghali\n",
            "-------\n",
            "Original      : رفاعة رافع الطهطاوي\n",
            "Transliterated: refa'a rafi tahawi\n",
            "Ground truth  : Rifa'a al-Tahtawi\n",
            "-------\n",
            "Original      : سعد زغلول‎\n",
            "Transliterated: saad zaghloul\n",
            "Ground truth  : Saad Zaghloul\n",
            "-------\n",
            "Original      : فاروق الباز‎\n",
            "Transliterated: faruq elbaz\n",
            "Ground truth  : Farouk El-Baz\n",
            "-------\n",
            "Original      : يعيش بن إبراهيم بن يوسف بن سماك الأموي الأندلسي\n",
            "Transliterated: yaish benn ebrahim benn yusaf benn smack alamoi andolsi\n",
            "Ground truth  : Abū ʿAbdallāh Yaʿīsh ibn Ibrāhīm ibn Yūsuf ibn Simāk al-Andalusī al-Umawī\n",
            "-------\n",
            "Original      : أحمد حسن زويل‎\n",
            "Transliterated: ahmed hasan zoel\n",
            "Ground truth  : Ahmed Hassan Zewail\n",
            "-------\n",
            "Original      : عبد الواحد الوكيل‎\n",
            "Transliterated: abdellahahahahahahahahahaha waahid alokel\n",
            "Ground truth  : Abdel-Wahed El-Wakil\n",
            "-------\n",
            "Original      : سعاد العامري‎\n",
            "Transliterated: souad amiry\n",
            "Ground truth  : Suad Amiry\n",
            "-------\n",
            "Original      : أحمد بن فضلان بن العباس بن راشد بن حماد‎\n",
            "Transliterated: ahmed benn fadlan benn alabbass benn rashed benn hamad\n",
            "Ground truth  : Aḥmad ibn Faḍlān ibn al-ʿAbbās ibn Rāšid ibn Ḥammād\n",
            "-------\n",
            "Original      : أحمد بن ماجد\n",
            "Transliterated: ahmed benn majed\n",
            "Ground truth  : Ahmad ibn Mājid\n",
            "-------\n",
            "Original      : عباس محمود العقاد‎\n",
            "Transliterated: abbass mahmud alagad\n",
            "Ground truth  : Abbas Mahmoud al-Aqqad\n",
            "-------\n",
            "Original      : ٱمْرُؤ ٱلْقَيْس جُنْدُح ٱبْن حُجْر ٱلْكِنْدِيّ‎\n",
            "Transliterated: amara algais jandah ibnu hajar kindi\n",
            "Ground truth  : Imru' al-Qais Junduh bin Hujr al-Kindi \n",
            "-------\n",
            "Original      : أبو القاسم خلف بن العباس الزهراوي\n",
            "Transliterated: abo qasim alalaf benn alabbass zahrawi\n",
            "Ground truth  : Abū al-Qāsim Khalaf ibn al-'Abbās al-Zahrāwī al-Ansari\n",
            "-------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}